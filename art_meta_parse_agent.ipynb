{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "KCTIISFU7B0T",
      "metadata": {
        "id": "KCTIISFU7B0T"
      },
      "source": [
        "# TODO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0Uc8Kx6vMXen",
      "metadata": {
        "id": "0Uc8Kx6vMXen"
      },
      "outputs": [],
      "source": [
        "#과제\n",
        "#State와 결과값을 각기 유지"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ggvlsBi9TuGI",
      "metadata": {
        "id": "ggvlsBi9TuGI"
      },
      "source": [
        "# Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7FfaZbbFZi2jJtGUiVXTnszn",
      "metadata": {
        "id": "7FfaZbbFZi2jJtGUiVXTnszn",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# %pip install -q -U langchain langchain_community langchain_core langchain_openai faiss-cpu langchain_anthropic langgraph langchain_google_community langchain_google_vertexai langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ylmPnNdGUEMo",
      "metadata": {
        "id": "ylmPnNdGUEMo"
      },
      "outputs": [],
      "source": [
        "#수집할 아티스트 데이터\n",
        "we_art_name = 'FROMIS_9'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sclt1-zoC38A",
      "metadata": {
        "id": "sclt1-zoC38A"
      },
      "source": [
        "## LLM 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd03b94d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install --quiet --upgrade google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "01cea84c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import re\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "import faiss\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_google_vertexai import VertexAIEmbeddings\n",
        "from langchain_google_community import BigQueryVectorStore\n",
        "from langchain_google_community.bigquery import BigQueryLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "\n",
        "import vertexai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0cefca77",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_google_vertexai import ChatVertexAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import ConfigurableField\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "544a83ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from pydantic  import BaseModel, Field\n",
        "from operator import itemgetter\n",
        "\n",
        "from time import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd395954",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from pydantic import BaseModel, Field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "mGPMuFVDC6vL",
      "metadata": {
        "id": "mGPMuFVDC6vL"
      },
      "outputs": [],
      "source": [
        "#API KEY 셋팅\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "#LLM 모델 생성 with Config\n",
        "\n",
        "llm = (\n",
        "    ChatAnthropic(model_name='claude-3-5-sonnet-20240620')\n",
        "    .configurable_alternatives(\n",
        "        ConfigurableField(\n",
        "            id = 'llm',\n",
        "            name=\"LLM Model\",\n",
        "            description=\"The base LLM model\",\n",
        "        ),\n",
        "        default_key=\"claude3_5_sonnet\",\n",
        "        claude3_haiku=ChatAnthropic(model_name='claude-3-haiku-20240307'),\n",
        "        gpt4o_mini = ChatOpenAI(model = 'gpt-4o-mini'),\n",
        "        gpt3_5 = ChatOpenAI(model = 'gpt-3.5-turbo'),\n",
        "        gemini_flash = ChatVertexAI(model=\"gemini-1.5-flash-001\")\n",
        "    )\n",
        "    .configurable_fields(\n",
        "        temperature=ConfigurableField(\n",
        "            id=\"temperature\",\n",
        "            name=\"LLM Temperature\",\n",
        "            description=\"The temperature of the LLM\",\n",
        "        ),\n",
        "        max_tokens = ConfigurableField(\n",
        "            id=\"max_token\",\n",
        "            name=\"Maximum input Tokens\",\n",
        "            description=\"Maximum limit of input Tokens\",\n",
        "        ),\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dpf38aBvDfbP",
      "metadata": {
        "id": "dpf38aBvDfbP"
      },
      "source": [
        "## LangSmith 셋팅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4KkDwYiVDfK_",
      "metadata": {
        "id": "4KkDwYiVDfK_"
      },
      "outputs": [],
      "source": [
        "os.environ['LANGCHAIN_API_KEY'] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-academy\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "r6SLLG3m5d-x",
      "metadata": {
        "id": "r6SLLG3m5d-x"
      },
      "source": [
        "# 위키문서 로드 & 벡터스토어 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "h1jiItnS5daq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 11395,
          "status": "ok",
          "timestamp": 1728996073804,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "h1jiItnS5daq",
        "outputId": "a97f6eef-c3ff-4ac6-fa37-cc60aec2e076"
      },
      "outputs": [],
      "source": [
        "#저장된 문서 불러오기\n",
        "PROJECT_ID = \"wev-dev-analytics\"\n",
        "REGION = \"asia-northeast3\"\n",
        "DATASET_ID = \"namu_wiki\"\n",
        "TABLE_ID = \"artists\"\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "# OpenAI의 \"text-embedding-3-large\" 모델을 사용하여 1024차원의 임베딩을 생성하는 객체를 초기화합니다.\n",
        "# embeddings_docs = OpenAIEmbeddings(model=\"text-embedding-3-small\", dimensions=1024, api_key = os.environ[\"OPENAI_API_KEY\"])\n",
        "embeddings_docs = VertexAIEmbeddings(model_name=\"text-multilingual-embedding-002\")\n",
        "\n",
        "\n",
        "#임베딩할 문서 로더\n",
        "bigquery_loader = BigQueryLoader(\n",
        "    query=f\"\"\"\n",
        "        SELECT *\n",
        "             , json_value(metadata, '$.parent_page_toc_item') as parent_paper\n",
        "        FROM `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`\n",
        "        WHERE artist_info = \"{we_art_name}\"\n",
        "    \"\"\",\n",
        "    project=PROJECT_ID,\n",
        "    page_content_columns = \"page_content\", #벡터화할 텍스트 지정\n",
        "    metadata_columns  = [\"artist_info\",\"toc_item\",\"abs_page_toc_item\",\"parent_paper\",\"page_url\"] #메타데이터 지정\n",
        ")\n",
        "\n",
        "documents = bigquery_loader.load()\n",
        "\n",
        "#Splitter 생성\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "  # 나무위키 한 문단에 적절하다고 판단하는 청크 사이즈\n",
        "  chunk_size=700,\n",
        "  # 적절하다고 생각하는 overlap\n",
        "  chunk_overlap=150,\n",
        "  # 문자열 길이를 계산하는 함수를 지정합니다.\n",
        "  length_function=len,\n",
        "  # 구분자로 정규식을 사용할지 여부를 설정합니다.\n",
        "  is_separator_regex=False,\n",
        ")\n",
        "\n",
        "book = []\n",
        "\n",
        "# metadata를 page_content에 포함시키는 함수\n",
        "def concatenate_metadata_into_content(doc):\n",
        "    doc.page_content = f\"{doc.metadata['abs_page_toc_item']}\\n\\n\"+re.sub('page_content:', '', f\"{doc.page_content}\")\n",
        "    return doc\n",
        "\n",
        "#Chunking with abs_page_toc_item\n",
        "for element in documents:\n",
        "  # text_splitter를 사용하여 텍스트를 문서로 분할합니다.\n",
        "  papers = text_splitter.create_documents(texts = [element.page_content], metadatas = [element.metadata])\n",
        "  #papers.page_content 앞에 element.metadata[abs_page_toc_item] concat하기\n",
        "  book.extend(list(map(concatenate_metadata_into_content, papers)))\n",
        "\n",
        "\n",
        "#FAISS 벡터스토어 생성\n",
        "index = faiss.IndexFlatL2(len(embeddings_docs.embed(texts = [\"hello world\"], embeddings_task_type = 'RETRIEVAL_DOCUMENT')))\n",
        "\n",
        "##Sementic Search를 위한 Chunking 문서 임베딩\n",
        "vstr = FAISS(\n",
        "    embedding_function=embeddings_docs,\n",
        "    index=index,\n",
        "    docstore= InMemoryDocstore(),\n",
        "    index_to_docstore_id={}\n",
        ")\n",
        "\n",
        "# Convert documents to embeddings and store them\n",
        "# vstr.add_documents(documents=book)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "Xx9gZ3VvAd9D",
      "metadata": {
        "id": "Xx9gZ3VvAd9D"
      },
      "outputs": [],
      "source": [
        "def get_chain(model, retriever = None, parser = StrOutputParser()):\n",
        "    # 시스템 프롬프트 설정\n",
        "    system_prompt = SystemMessagePromptTemplate.from_template(\n",
        "        \"\"\"\n",
        "        You are a biographical dictionary agent. Your sole purpose is to provide factual information based strictly on the provided <Context>.\n",
        "\n",
        "        - **Behavior:**\n",
        "          - Be friendly and helpful without being overly chatty.\n",
        "          - Do not use any prior knowledge beyond the <Context>.\n",
        "          - If the answer cannot be found within the <Context>, respond with \"None\"\n",
        "\n",
        "        - **Input Structure:**\n",
        "          - The input will be provided using the following tags:\n",
        "            ```\n",
        "            <Context>\n",
        "            ...\n",
        "            </Context>\n",
        "            <Question>\n",
        "            ...\n",
        "            </Question>\n",
        "            <Format>\n",
        "            ...\n",
        "            </Format>\n",
        "            ```\n",
        "\n",
        "        - **Response Guidelines:**\n",
        "          - **Do not** include the `<Context>`, `<Question>`, or `<Format>` tags in your response.\n",
        "          - **Only** provide the answer as specified in the `<Format>` section.\n",
        "          - Ensure the response strictly adheres to the format provided.\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # Runnable로 채워질 부분\n",
        "    human_prompt = HumanMessagePromptTemplate.from_template(\n",
        "        \"\"\"<Context>{context}</Context>\\n\\n\n",
        "        <Question>{question}</Question>\\n\\n\n",
        "        <Format>{format_instructions}</Format>\"\"\"\n",
        "    )\n",
        "    prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
        "    if isinstance(parser, JsonOutputParser):\n",
        "      prompt = prompt.partial(format_instructions=parser.get_format_instructions())\n",
        "\n",
        "    def concat_docs(docs) -> str: # retriever가 반환한 모든 Document들의 page_content를 하나의 단일 string으로 붙여주는 함수\n",
        "        return \"\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    chain = {}\n",
        "\n",
        "    # 기본 메타데이터는 리트리버 사용하지 않고 지정된 문서를 Context로 전달\n",
        "    if retriever is None:\n",
        "      chain = {\n",
        "            'context': itemgetter('context') | RunnablePassthrough()\n",
        "          , 'question': itemgetter('question') | RunnablePassthrough()\n",
        "      } | prompt | model | parser\n",
        "    else:\n",
        "      chain = {\n",
        "            \"context\" : itemgetter('question') | retriever | concat_docs\n",
        "          , \"question\" : itemgetter('question') | RunnablePassthrough()\n",
        "      } | prompt| model | parser\n",
        "\n",
        "    return chain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "v3mMFcXhBbHk",
      "metadata": {
        "id": "v3mMFcXhBbHk"
      },
      "source": [
        "# 질문 Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "fxBKMjr9BdS4",
      "metadata": {
        "id": "fxBKMjr9BdS4"
      },
      "outputs": [],
      "source": [
        "# 지시: Answer the question\n",
        "# 형식: JsonOutputParser\n",
        "# 맥락: context (documents or retriever)\n",
        "# 역할: SystemPrompt\n",
        "\n",
        "# 원하는 데이터 구조를 정의합니다.\n",
        "class Ctry(BaseModel):\n",
        "    ctry: list = Field(description=\"ISO 3166-1 alpha-2 형식, 최소 1개 이상\")\n",
        "\n",
        "class Gender(BaseModel):\n",
        "    gender: str = Field(description=\"남성과 관련된 단어가 나오면 '남성'으로, 여성과 관련된 단어가 나오면 '여성'으로 출력\")\n",
        "\n",
        "class Genre(BaseModel):\n",
        "    genre: list = Field(description=\"문서 내 순서를 따르고, 모든 장르를 출력\")\n",
        "\n",
        "class Label(BaseModel):\n",
        "    label_name: list = Field(description=\"고유명사만 출력하고 **괄호와 괄호 안의 내용은 반드시 제거**, 2개 이상인 경우 솔로 활동을 앞 순서에, 그룹 활동은 뒷 순서에 배치해주세요.\")\n",
        "\n",
        "class Debut(BaseModel):\n",
        "    debut_date: str = Field(description=\"'yyyy-MM-dd' 형식으로 출력\")\n",
        "\n",
        "class MemberInfo(BaseModel):\n",
        "    member_list: list = Field(description=\"문서 내 순서를 따른다.\")\n",
        "\n",
        "class MemberNumber(BaseModel):\n",
        "    member_cnt: int = Field(description=\"숫자로만 출력하세요.\")\n",
        "\n",
        "class Fandom(BaseModel):\n",
        "    fandom_name: str = Field(description=\"고유명사만 출력하고, 반복되는 문자열은 하나로 합쳐주세요.\")\n",
        "\n",
        "\n",
        "# 컬럼명 & 컬럼용 질문 Dict 생성\n",
        "singer_queries = {\n",
        "    \"ctry\":{\n",
        "        \"question\":\"활동 국가는?\",\n",
        "        \"format\":JsonOutputParser(pydantic_object=Ctry),\n",
        "      }, #???\n",
        "    \"gender\":{\"question\":\"인물의 성별을 유추하세요. (힌트: 보이그룹, 걸그룹, 혼성듀오, 형, 누나, 오빠)\",\n",
        "              \"format\":JsonOutputParser(pydantic_object=Gender),\n",
        "              \"loc\":\"개요\" #(솔로)PROFILE > 가족 or (그룹)개요\n",
        "    },\n",
        "    \"genre\": {\"question\":\"음악 장르는?\",\n",
        "              # \"format\":\"가장 관련성이 큰 장르 2개로 정리해줘. [장르 A, 장르 B]와 같이 쉼표로 구분된 한 줄로 출력하세요.\",\n",
        "              \"format\":JsonOutputParser(pydantic_object=Genre),\n",
        "              \"loc\":\"PROFILE\" #PROFILE > 장르\n",
        "    },\n",
        "    \"label_name\": {\"question\":\"엔터테인먼트 회사명은?\",\n",
        "                   \"format\":JsonOutputParser(pydantic_object=Label),\n",
        "                   \"loc\":\"PROFILE\" #PROFILE > 소속사\n",
        "    },\n",
        "    \"debut_date\": {\"question\":\"데뷔일자는?\",\n",
        "                   \"format\":JsonOutputParser(pydantic_object=Debut),\n",
        "                   \"loc\":\"PROFILE\" #PROFILE > 데뷔일\n",
        "    },\n",
        "    \"member_list\": {\"question\":\"문서에서 사람 이름을 모두 나열해주세요\",\n",
        "                    \"format\":JsonOutputParser(pydantic_object=MemberInfo),\n",
        "                    \"loc\":\"멤버\" #멤버\n",
        "    },\n",
        "    \"member_num\": {\"question\":\"문서에서 사람은 총 몇 명인가요?\",\n",
        "                   \"format\":JsonOutputParser(pydantic_object=MemberNumber),\n",
        "                   \"loc\":\"멤버\" #멤버\n",
        "    },\n",
        "    \"fandom_name\": {\"question\":\"팬덤명은?\",\n",
        "                    \"format\":JsonOutputParser(pydantic_object=Fandom),\n",
        "                    \"loc\":\"PROFILE\" #PROFILE > 팬덤\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8xr42eJQT-QL",
      "metadata": {
        "id": "8xr42eJQT-QL"
      },
      "source": [
        "# Tool 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "gqt-h-kX8LbL",
      "metadata": {
        "id": "gqt-h-kX8LbL"
      },
      "outputs": [],
      "source": [
        "#툴은 크게 4개\n",
        "#0. 전달받은 문서에서 메타데이터를 생성하는 툴\n",
        "#1. 프로필에서 메타데이터 생성\n",
        "## 1.1. 0.에 프로필을 전달\n",
        "#2. 개요/멤버/팬덤 문서에서 메타데이터 생성\n",
        "## 2.1. 0.에 개요/멤버/팬덤을 각각 전달\n",
        "## 2.2. 2.1.에서 생성한 3개의 df를 머지\n",
        "#3. 리트리버에서 메타데이터 생성\n",
        "## 3.1. 흠...\n",
        "#4. 1~3의 결과를 종합해서 최종 데이터를 만드는 툴\n",
        "\n",
        "#나중에는...\n",
        "#아티스트명을 입력받아서 이 아티스트가 가수/배우/그 외에 따라서 질문 set을 다르게 가져와야함."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uZS9rQIRA4t_",
      "metadata": {
        "id": "uZS9rQIRA4t_"
      },
      "source": [
        "## 전달받은 문서에서 메타데이터를 생성하는 툴"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "6E2Hrqz_A7VY",
      "metadata": {
        "id": "6E2Hrqz_A7VY"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def answer_to_df(q_dict, docs):\n",
        "\n",
        "    answer_list = []\n",
        "\n",
        "    for key, val in q_dict.items():\n",
        "        #Chain 생성\n",
        "        rag_chain = get_chain(model = llm, parser = val['format'])\n",
        "        rag_chain = rag_chain.with_config(configurable={\n",
        "            # \"llm\": \"gpt4o_mini\", #model_name,\n",
        "            \"llm\":\"gemini_flash\",\n",
        "            \"temparature\": 0, #temp,\n",
        "            \"max_tokens\": None #max_tokens\n",
        "        })\n",
        "\n",
        "        answer = rag_chain.invoke({\"question\": val['question'], \"context\":docs})\n",
        "        answer_list.append(answer)\n",
        "\n",
        "    # 각 딕셔너리 병합\n",
        "    merged_dict = {}\n",
        "    for item in answer_list:\n",
        "        merged_dict.update(item)\n",
        "\n",
        "    # 병합된 딕셔너리를 DataFrame으로 변환\n",
        "    result = pd.DataFrame([merged_dict])\n",
        "    # print(result)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IVp-RbIDA0OL",
      "metadata": {
        "id": "IVp-RbIDA0OL"
      },
      "source": [
        "## 지정된 문서 원문에서 메타데이터를 생성하는 Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "VngUDoqtLUt6",
      "metadata": {
        "id": "VngUDoqtLUt6"
      },
      "outputs": [],
      "source": [
        "def df_from_origin(q_dict, origin):\n",
        "    try:\n",
        "      #RunnableParellel로 변경할 부분\n",
        "      df = answer_to_df(q_dict, \"\\n\\n\".join([doc.page_content for doc in documents if doc.metadata['abs_page_toc_item'] == origin]))\n",
        "    except Exception as e:\n",
        "      #에러가 발생하는 경우 빈 데이터프레임을 반환\n",
        "      print(e)\n",
        "      df = pd.DataFrame(columns = ['ctry','gender','genre','label_name','debut_date','member_list','member_num','fandom_name'])\n",
        "    finally:\n",
        "      return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mbJxc3wXb-Dt",
      "metadata": {
        "id": "mbJxc3wXb-Dt"
      },
      "source": [
        "## 리트리빙해서 메타데이터를 생성하는 Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "PvWEi3HUn8JB",
      "metadata": {
        "id": "PvWEi3HUn8JB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × python setup.py egg_info did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [1 lines of output]\n",
            "      ERROR: Can not execute `setup.py` since setuptools is not available in the build environment.\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "error: metadata-generation-failed\n",
            "\n",
            "× Encountered error while generating package metadata.\n",
            "╰─> See above for output.\n",
            "\n",
            "note: This is an issue with the package mentioned above, not pip.\n",
            "hint: See above for details.\n",
            "\n",
            "[notice] A new release of pip is available: 24.1.2 -> 24.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# 패키지 업데이트\n",
        "# %pip install -qU langchain-teddynote"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "eLnwgvXOhZ6B",
      "metadata": {
        "id": "eLnwgvXOhZ6B"
      },
      "outputs": [],
      "source": [
        "#단순 리트리빙이 아니라, 요약을 하던지, 리랭크를 하던지 해야함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iYGLvMpImQSV",
      "metadata": {
        "id": "iYGLvMpImQSV"
      },
      "outputs": [],
      "source": [
        "# 문서를 예쁘게 출력하기 위한 도우미 함수\n",
        "def pretty_print_docs(docs):\n",
        "    print(\n",
        "        f\"\\n{'-' * 100}\\n\".join(\n",
        "            [f\"문서 {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "9jj2bPbzdKX-",
      "metadata": {
        "id": "9jj2bPbzdKX-"
      },
      "outputs": [],
      "source": [
        "ret = vstr.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "reMJxayrmXcw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 335,
          "status": "ok",
          "timestamp": 1728996078516,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "reMJxayrmXcw",
        "outputId": "d5cce494-3a7a-4207-fdc6-522b45dac3e7"
      },
      "outputs": [],
      "source": [
        "# 쿼리에 질문을 정의하고 관련 문서를 검색합니다.\n",
        "docs = ret.invoke(\"노지선의 나이는?\")\n",
        "\n",
        "# 검색된 문서를 예쁘게 출력합니다.\n",
        "pretty_print_docs(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eCTKmZKhmwh5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 6597,
          "status": "ok",
          "timestamp": 1728865646717,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "eCTKmZKhmwh5",
        "outputId": "1d9d68f7-3c26-4554-e08f-a7bd62f32ad5"
      },
      "outputs": [],
      "source": [
        "from langchain_teddynote.document_compressors import LLMChainExtractor\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "\n",
        "# from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")  # OpenAI 언어 모델 초기화\n",
        "\n",
        "# LLM을 사용하여 문서 압축기 생성\n",
        "compressor = LLMChainExtractor.from_llm(llm)\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    # 문서 압축기와 리트리버를 사용하여 컨텍스트 압축 리트리버 생성\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=ret,\n",
        ")\n",
        "\n",
        "pretty_print_docs(ret.invoke(\"노지선의 나이는?\"))\n",
        "\n",
        "print(\"=========================================================\")\n",
        "print(\"============== LLMChainExtractor 적용 후 ==================\")\n",
        "\n",
        "compressed_docs = (\n",
        "    compression_retriever.invoke(  # 컨텍스트 압축 리트리버를 사용하여 관련 문서 검색\n",
        "        \"노지선의 나이는?\"\n",
        "    )\n",
        ")\n",
        "pretty_print_docs(compressed_docs)  # 검색된 문서를 예쁘게 출력\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IzzpEPS4o44W",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 4046,
          "status": "ok",
          "timestamp": 1728865729892,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "IzzpEPS4o44W",
        "outputId": "9ba1a03d-e5d9-48a4-8d71-e33320eb4bca"
      },
      "outputs": [],
      "source": [
        "from langchain_teddynote.document_compressors import LLMChainFilter\n",
        "\n",
        "# LLM을 사용하여 LLMChainFilter 객체를 생성합니다.\n",
        "_filter = LLMChainFilter.from_llm(llm)\n",
        "\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    # LLMChainFilter와 retriever를 사용하여 ContextualCompressionRetriever 객체를 생성합니다.\n",
        "    base_compressor=_filter,\n",
        "    base_retriever=ret,\n",
        ")\n",
        "\n",
        "compressed_docs = compression_retriever.invoke(\n",
        "    # 쿼리\n",
        "    \"노지선의 나이는?\"\n",
        ")\n",
        "pretty_print_docs(compressed_docs)  # 압축된 문서를 예쁘게 출력합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "opHVo5YMd7rX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1181,
          "status": "ok",
          "timestamp": 1728866522574,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "opHVo5YMd7rX",
        "outputId": "cd9c0f2e-94da-4587-f435-70532c2e5f09"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# 유사도 임계값이 0.76인 EmbeddingsFilter 객체를 생성합니다.\n",
        "embeddings_filter = EmbeddingsFilter(embeddings=embeddings_docs, similarity_threshold=0.3)\n",
        "\n",
        "# 기본 압축기로 embeddings_filter를, 기본 검색기로 retriever를 사용하여 ContextualCompressionRetriever 객체를 생성합니다.\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=embeddings_filter, base_retriever=ret\n",
        ")\n",
        "\n",
        "# ContextualCompressionRetriever 객체를 사용하여 관련 문서를 검색합니다.\n",
        "compressed_docs = compression_retriever.invoke(\n",
        "    # 쿼리\n",
        "    \"노지선의 나이는?\"\n",
        ")\n",
        "# 압축된 문서를 예쁘게 출력합니다.\n",
        "pretty_print_docs(compressed_docs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wBBsQzpXrChs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 4159,
          "status": "ok",
          "timestamp": 1728866621610,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "wBBsQzpXrChs",
        "outputId": "4de0dcdd-2d3f-4cc1-eb6d-dc7e8a9bd715"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.document_compressors import DocumentCompressorPipeline, EmbeddingsFilter\n",
        "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "# 문자 기반 텍스트 분할기를 생성하고, 청크 크기를 300으로, 청크 간 중복을 0으로 설정합니다.\n",
        "splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
        "\n",
        "# 임베딩을 사용하여 중복 필터를 생성합니다.\n",
        "redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings_docs)\n",
        "\n",
        "# 임베딩을 사용하여 관련성 필터를 생성하고, 유사도 임계값을 0.86으로 설정합니다.\n",
        "relevant_filter = EmbeddingsFilter(embeddings=embeddings_docs, similarity_threshold=0.3)\n",
        "\n",
        "pipeline_compressor = DocumentCompressorPipeline(\n",
        "    # 문서 압축 파이프라인을 생성하고, 분할기, 중복 필터, 관련성 필터, LLMChainExtractor를 변환기로 설정합니다.\n",
        "    transformers=[\n",
        "        splitter,\n",
        "        redundant_filter,\n",
        "        relevant_filter,\n",
        "        LLMChainExtractor.from_llm(llm),\n",
        "    ]\n",
        ")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    # 기본 압축기로 pipeline_compressor를 사용하고, 기본 검색기로 retriever를 사용하여 ContextualCompressionRetriever를 초기화합니다.\n",
        "    base_compressor=pipeline_compressor,\n",
        "    base_retriever=ret,\n",
        ")\n",
        "\n",
        "compressed_docs = compression_retriever.invoke(\n",
        "    # 쿼리\n",
        "    \"노지선의 나이는?\"\n",
        ")\n",
        "# 압축된 문서를 예쁘게 출력합니다.\n",
        "pretty_print_docs(compressed_docs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sqBnMqezfmfz",
      "metadata": {
        "id": "sqBnMqezfmfz"
      },
      "outputs": [],
      "source": [
        "chain = itemgetter('question') | ret | concat_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K-QVHKIBfw6m",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "executionInfo": {
          "elapsed": 633,
          "status": "ok",
          "timestamp": 1727742980645,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "K-QVHKIBfw6m",
        "outputId": "eeea8075-88ca-4dfe-de64-c0d88536a216"
      },
      "outputs": [],
      "source": [
        "chain.invoke({'question':'음악 장르는?'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MinImK5cfFkL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1727742851407,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "MinImK5cfFkL",
        "outputId": "967722af-65fe-4c7a-cbf2-4cf4bb68147c"
      },
      "outputs": [],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vU8k0YThe96A",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 305,
          "status": "ok",
          "timestamp": 1727742613988,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "vU8k0YThe96A",
        "outputId": "b0923bc4-6da8-4f6a-d576-71c631399a0a"
      },
      "outputs": [],
      "source": [
        "type(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eedd00UZcJwR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 5,
          "status": "ok",
          "timestamp": 1727741903148,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "eedd00UZcJwR",
        "outputId": "b60776d6-bff4-4976-f2ba-271f8cc59b4e"
      },
      "outputs": [],
      "source": [
        "def df_from_retriever(q_dict):\n",
        "  ret = vstr.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "js7vmRb5Hm6t",
      "metadata": {
        "id": "js7vmRb5Hm6t"
      },
      "outputs": [],
      "source": [
        "df = df_from_origin(singer_queries, '개요')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MV_4PXi5UkFc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1727740827064,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "MV_4PXi5UkFc",
        "outputId": "52e1eb58-70bb-4153-d6fc-88638dcd2e5a"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5o1-xNxx8GOT",
      "metadata": {
        "id": "5o1-xNxx8GOT"
      },
      "source": [
        "# Agent 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "p14hr2HZ7Qb2",
      "metadata": {
        "id": "p14hr2HZ7Qb2"
      },
      "outputs": [],
      "source": [
        "#Workflow\n",
        "##1. (미구현) 아티스트의 성격 분류 / 가수(현재 구현 ver), 배우, 셀럽\n",
        "##2. 총 5개 (프로필 / 개요 / 멤버 / 팬덤 + 전체 문서 리트리빙)의 툴을 실행\n",
        "##2.1. 각 결과값으로 pandas dataframe을 생성\n",
        "##2.2. 프로필 / 개요 / 멤버 / 팬덤은 Map-reduce 적용 검토\n",
        "##3. 5개 문서 결과값에 대해 가중치를 적용해서 dataframe merge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "4FP9FZkVzIcH",
      "metadata": {
        "id": "4FP9FZkVzIcH"
      },
      "outputs": [],
      "source": [
        "llm = llm.with_config(configurable={\n",
        "            # \"llm\": \"gpt4o_mini\", #model_name,\n",
        "            \"llm\":\"gemini_flash\",\n",
        "            \"temparature\": 0, #temp,\n",
        "            \"max_tokens\": None #max_tokens\n",
        "        })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "3ThxdYlDiYKN",
      "metadata": {
        "id": "3ThxdYlDiYKN"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from typing_extensions import TypedDict\n",
        "from pydantic import BaseModel, Field\n",
        "import operator\n",
        "from typing import Annotated\n",
        "from langgraph.graph import MessagesState\n",
        "\n",
        "class ArtistMeta(TypedDict):\n",
        "    ctry: list #= Field(description=\"ISO 3166-1 alpha-2 형식, 최소 1개 이상\")\n",
        "    gender: str #= Field(description=\"남성과 관련된 단어가 나오면 '남성'으로, 여성과 관련된 단어가 나오면 '여성'으로 출력\")\n",
        "    genre: list #= Field(description=\"문서 내 순서를 따르고, 모든 장르를 출력\")\n",
        "    label_name: list #= Field(description=\"고유명사만 출력하고 **괄호와 괄호 안의 내용은 반드시 제거**, 2개 이상인 경우 솔로 활동을 앞 순서에, 그룹 활동은 뒷 순서에 배치해주세요.\")\n",
        "    debut_date: str #= Field(description=\"'yyyy-MM-dd' 형식으로 출력\")\n",
        "    member_list: list #= Field(description=\"문서 내 순서를 따른다.\")\n",
        "    member_cnt: int #= Field(description=\"숫자로만 출력하세요.\")\n",
        "    fandom_name: str #= Field(description=\"고유명사만 출력하고, 반복되는 문자열은 하나로 합쳐주세요.\")\n",
        "\n",
        "class ArtistState(TypedDict):\n",
        "    qnf: dict #= Field(description=\"아티스트에 대한 특정 질문\")\n",
        "    doc: str #= Field(description=\"아티스트 데이터를 찾아볼 문서\")\n",
        "    meta: Annotated[List[dict], operator.add]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "h6_nn4Vq0QND",
      "metadata": {
        "id": "h6_nn4Vq0QND"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "def answer_from_origin(state: ArtistState):\n",
        "\n",
        "  question = state['qnf']['question']\n",
        "  format = state['qnf']['format']\n",
        "  doc = state['doc']\n",
        "  meta = state['meta']\n",
        "\n",
        "  # 시스템 프롬프트 설정\n",
        "  system_prompt = SystemMessagePromptTemplate.from_template(\n",
        "      \"\"\"\n",
        "      You are a biographical dictionary agent. Your sole purpose is to provide factual information based strictly on the provided <Context>.\n",
        "\n",
        "      - **Behavior:**\n",
        "        - Be friendly and helpful without being overly chatty.\n",
        "        - Do not use any prior knowledge beyond the <Context>.\n",
        "        - If the answer cannot be found within the <Context>, respond with \"None\"\n",
        "\n",
        "      - **Input Structure:**\n",
        "        - The input will be provided using the following tags:\n",
        "          ```\n",
        "          <Context>\n",
        "          ...\n",
        "          </Context>\n",
        "          <Question>\n",
        "          ...\n",
        "          </Question>\n",
        "          <Format>\n",
        "          ...\n",
        "          </Format>\n",
        "          ```\n",
        "\n",
        "      - **Response Guidelines:**\n",
        "        - **Do not** include the `<Context>`, `<Question>`, or `<Format>` tags in your response.\n",
        "        - **Only** provide the answer as specified in the `<Format>` section.\n",
        "        - Ensure the response strictly adheres to the format provided.\n",
        "      \"\"\"\n",
        "  )\n",
        "\n",
        "  human_prompt = HumanMessagePromptTemplate.from_template(\n",
        "      \"\"\"<Context>{context}</Context>\\n\\n\n",
        "      <Question>{question}</Question>\\n\\n\n",
        "      <Format>{format_instructions}</Format>\"\"\"\n",
        "  )\n",
        "  prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
        "  if isinstance(format, JsonOutputParser):\n",
        "    prompt = prompt.partial(format_instructions=format.get_format_instructions())\n",
        "\n",
        "  chain = {\n",
        "          'context': itemgetter('context') | RunnablePassthrough()\n",
        "        , 'question': itemgetter('question') | RunnablePassthrough()\n",
        "    } | prompt | llm | format\n",
        "\n",
        "  answer = chain.invoke({\"question\": question, \"context\":doc})\n",
        "  print(answer)\n",
        "\n",
        "  # meta.update(answer)\n",
        "  return {\"meta\":[answer]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "r0ltNvoXA0_m",
      "metadata": {
        "id": "r0ltNvoXA0_m"
      },
      "outputs": [],
      "source": [
        "from langgraph.constants import Send\n",
        "\n",
        "def init_graph(state:ArtistState):\n",
        "  pass\n",
        "\n",
        "def split_question(state:ArtistState):\n",
        "  qnf = singer_queries\n",
        "  doc = \"\\n\\n\".join([doc.page_content for doc in documents if doc.metadata['abs_page_toc_item'] == \"PROFILE\"])\n",
        "  return [Send(\"answer_from_origin\", {\"qnf\":qnf[key], \"doc\":doc, \"meta\": {}}) for key, value in qnf.items()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "ZgloiWZQPqOB",
      "metadata": {
        "id": "ZgloiWZQPqOB"
      },
      "outputs": [],
      "source": [
        "#질문Set과 특정 페이지 원문이 input\n",
        "#원문에 대해 질문Set의 각각을 별도로 질문\n",
        "#meta에 list로 merge한 것이 output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "u0vyVGMdB1Ht",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "executionInfo": {
          "elapsed": 378,
          "status": "ok",
          "timestamp": 1728996098726,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "u0vyVGMdB1Ht",
        "outputId": "73f29bf8-4992-44ba-dc88-b298b4953f8b"
      },
      "outputs": [
        {
          "data": {
            "image/jpeg": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "from langgraph.graph import START, END, StateGraph\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# Add nodes and edges\n",
        "builder = StateGraph(ArtistState)\n",
        "\n",
        "#node\n",
        "builder.add_node(\"init_extract\", init_graph)\n",
        "builder.add_node(\"answer_from_origin\", answer_from_origin)\n",
        "\n",
        "#edge\n",
        "builder.add_edge(START, \"init_extract\")\n",
        "builder.add_conditional_edges(\"init_extract\", split_question, [\"answer_from_origin\"])\n",
        "builder.add_edge(\"answer_from_origin\", END)\n",
        "\n",
        "# Compile\n",
        "graph = builder.compile()\n",
        "\n",
        "# View\n",
        "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j55Rv-96B2OW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 2934,
          "status": "ok",
          "timestamp": 1728996102244,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "j55Rv-96B2OW",
        "outputId": "4eb16625-b882-4a44-fccd-da7872e2a3ab"
      },
      "outputs": [],
      "source": [
        "graph.invoke({\"meta\":[]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uVUE6rwtB2Ti",
      "metadata": {
        "id": "uVUE6rwtB2Ti"
      },
      "outputs": [],
      "source": [
        "#전체 그래프\n",
        "#질문Set과 documents가 input\n",
        "#List[Dict]가 output / 또는 pd.Dataframe이 output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "4Nxkj3emB2YW",
      "metadata": {
        "id": "4Nxkj3emB2YW"
      },
      "outputs": [],
      "source": [
        "class PageState(TypedDict):\n",
        "    page: str\n",
        "    hit: str\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    questions: dict\n",
        "    documents: list\n",
        "    meta: List[dict]\n",
        "    page_cond: Annotated[List[dict], operator.add]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mooKuB7YQahp",
      "metadata": {
        "id": "mooKuB7YQahp"
      },
      "outputs": [],
      "source": [
        "#check_origin\n",
        "#page명, documents가 인풋\n",
        "#page명, 존재를 다음 edge의 인풋으로 전달\n",
        "#다음 edge는 페이지 Not Found이면 return empty dict or dataframe인 노드로 보내고 아니면 answer_from_origin으로 보냄"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "T4oCFS9us5Zw",
      "metadata": {
        "id": "T4oCFS9us5Zw"
      },
      "outputs": [],
      "source": [
        "from langgraph.constants import Send\n",
        "\n",
        "def init_graph(state:GraphState):\n",
        "  # return {\"meta\":[]}\n",
        "  pass\n",
        "\n",
        "def split_origin_pages(state:GraphState):\n",
        "  origin_pages = [\"PROFILE\",\"개요\",\"멤버\",\"팬덤\",\"덕질\"]\n",
        "  return [Send(\"check_origin\", {\"page\":p}) for p in origin_pages]\n",
        "\n",
        "def check_origin(state:PageState):\n",
        "  page = state['page']\n",
        "  doc = \"\\n\\n\".join([doc.page_content for doc in documents if doc.metadata['abs_page_toc_item'] == page])\n",
        "  if doc != '':\n",
        "    print(\"page \"+page+\" is not None\")\n",
        "    return {\"page_cond\":[{\"page\":page, \"hit\":\"Y\"}]}\n",
        "  else:\n",
        "    print(\"page \"+page+\" is None\")\n",
        "    return {\"page_cond\":[{\"page\":page, \"hit\":\"N\"}]}\n",
        "\n",
        "def split_qna(state:GraphState):\n",
        "  page_cond = state['page_cond']\n",
        "  \n",
        "  for p in page_cond:\n",
        "    if p['hit'] == 'Y':\n",
        "      print(p['page'], ':Y')\n",
        "    else:\n",
        "      print(p['page'], ':n')\n",
        "\n",
        "  # doc = \"\\n\\n\".join([doc.page_content for doc in documents if doc.metadata['abs_page_toc_item'] == subj])\n",
        "  # return [Send(\"answer_from_origin\", {\"qnf\":qnf[key], \"doc\":doc, \"meta\": {}}) for key, value in qnf.items()]\n",
        "\n",
        "def merge_meta(state:ArtistState):\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kTwmcCJZ24RS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 303,
          "status": "ok",
          "timestamp": 1728991303484,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "kTwmcCJZ24RS",
        "outputId": "7232e166-cc6d-48d6-f9ab-dd6f77e65701"
      },
      "outputs": [],
      "source": [
        "for key, value in singer_queries.items():\n",
        "  print(key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "id": "jtALB4HauD68",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1728989618027,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "jtALB4HauD68",
        "outputId": "418fe27b-8c6d-4652-ff90-8cd93356bb61"
      },
      "outputs": [
        {
          "data": {
            "image/jpeg": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "from langgraph.graph import START, END, StateGraph\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# Add nodes and edges\n",
        "builder = StateGraph(GraphState)\n",
        "builder.add_node(\"init_extract\", init_graph)\n",
        "builder.add_node(\"check_origin\", check_origin)\n",
        "builder.add_node(\"split_qna\", split_qna)\n",
        "# builder.add_node(\"merge_meta\", merge_meta)\n",
        "builder.add_edge(START, \"init_extract\")\n",
        "builder.add_conditional_edges(\"init_extract\", split_origin_pages, [\"check_origin\"])\n",
        "builder.add_edge(\"check_origin\", \"split_qna\")\n",
        "builder.add_edge(\"split_qna\", END)\n",
        "\n",
        "# Compile\n",
        "memory = MemorySaver()\n",
        "# graph = builder.compile(checkpointer=memory)\n",
        "graph = builder.compile()\n",
        "\n",
        "# View\n",
        "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "PzJ0U9UvbzPW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1728989619401,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "PzJ0U9UvbzPW",
        "outputId": "f9b0140e-c92b-4577-c8ba-5dba8a1e0dab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page PROFILE is not None\n",
            "page 개요 is not None\n",
            "page 멤버 is not None\n",
            "page 덕질 is None\n",
            "page 팬덤 is None\n",
            "PROFILE :Y\n",
            "개요 :Y\n",
            "멤버 :Y\n",
            "팬덤 :n\n",
            "덕질 :n\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'meta': {},\n",
              " 'page_cond': [{'page': 'PROFILE', 'hit': 'Y'},\n",
              "  {'page': '개요', 'hit': 'Y'},\n",
              "  {'page': '멤버', 'hit': 'Y'},\n",
              "  {'page': '팬덤', 'hit': 'N'},\n",
              "  {'page': '덕질', 'hit': 'N'}]}"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph.invoke({\"meta\":{}})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0MNkVs5Juajg",
      "metadata": {
        "id": "0MNkVs5Juajg"
      },
      "outputs": [],
      "source": [
        "# Input\n",
        "# thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "# Run the graph until the first interruption\n",
        "# event = graph.invoke({\"qnf\":singer_queries['debut_date'], \"doc\":\"\\n\\n\".join([doc.page_content for doc in documents if doc.metadata['abs_page_toc_item'] == 'PROFILE']), \"meta\":{}})\n",
        "# event = graph.invoke({\"questions\":singer_queries, \"documents\":documents, \"meta\":{}})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "O63S9gh1jxqs",
      "metadata": {
        "id": "O63S9gh1jxqs"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import get_buffer_string\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# 컬럼명 & 컬럼용 질문 Dict 생성\n",
        "singer_queries = {\n",
        "    \"ctry\":{\n",
        "        \"question\":\"활동 국가는?\",\n",
        "        \"format\":JsonOutputParser(pydantic_object=Ctry),\n",
        "      }, #???\n",
        "    \"gender\":{\"question\":\"인물의 성별을 유추하세요. (힌트: 보이그룹, 걸그룹, 혼성듀오, 형, 누나, 오빠)\",\n",
        "              \"format\":JsonOutputParser(pydantic_object=Gender),\n",
        "              \"loc\":\"개요\" #(솔로)PROFILE > 가족 or (그룹)개요\n",
        "    },\n",
        "    \"genre\": {\"question\":\"음악 장르는?\",\n",
        "              # \"format\":\"가장 관련성이 큰 장르 2개로 정리해줘. [장르 A, 장르 B]와 같이 쉼표로 구분된 한 줄로 출력하세요.\",\n",
        "              \"format\":JsonOutputParser(pydantic_object=Genre),\n",
        "              \"loc\":\"PROFILE\" #PROFILE > 장르\n",
        "    },\n",
        "    \"label_name\": {\"question\":\"엔터테인먼트 회사명은?\",\n",
        "                   \"format\":JsonOutputParser(pydantic_object=Label),\n",
        "                   \"loc\":\"PROFILE\" #PROFILE > 소속사\n",
        "    },\n",
        "    \"debut_date\": {\"question\":\"데뷔일자는?\",\n",
        "                   \"format\":JsonOutputParser(pydantic_object=Debut),\n",
        "                   \"loc\":\"PROFILE\" #PROFILE > 데뷔일\n",
        "    },\n",
        "    \"member_list\": {\"question\":\"문서에서 사람 이름을 모두 나열해주세요\",\n",
        "                    \"format\":JsonOutputParser(pydantic_object=MemberInfo),\n",
        "                    \"loc\":\"멤버\" #멤버\n",
        "    },\n",
        "    \"member_num\": {\"question\":\"문서에서 사람은 총 몇 명인가요?\",\n",
        "                   \"format\":JsonOutputParser(pydantic_object=MemberNumber),\n",
        "                   \"loc\":\"멤버\" #멤버\n",
        "    },\n",
        "    \"fandom_name\": {\"question\":\"팬덤명은?\",\n",
        "                    \"format\":JsonOutputParser(pydantic_object=Fandom),\n",
        "                    \"loc\":\"PROFILE\" #PROFILE > 팬덤\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# Search query writing\n",
        "search_instructions = SystemMessage(content=f\"\"\"You will be given a conversation between an analyst and an expert.\n",
        "\n",
        "Your goal is to generate a well-structured query for use in retrieval and / or web-search related to the conversation.\n",
        "\n",
        "First, analyze the full conversation.\n",
        "\n",
        "Pay particular attention to the final question posed by the analyst.\n",
        "\n",
        "Convert this final question into a well-structured web search query\"\"\")\n",
        "\n",
        "def search_web(state: InterviewState):\n",
        "\n",
        "    \"\"\" Retrieve docs from web search \"\"\"\n",
        "\n",
        "    # Search query\n",
        "    structured_llm = llm.with_structured_output(SearchQuery)\n",
        "    search_query = structured_llm.invoke([search_instructions]+state['messages'])\n",
        "\n",
        "    # Search\n",
        "    search_docs = tavily_search.invoke(search_query.search_query)\n",
        "\n",
        "     # Format\n",
        "    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
        "        [\n",
        "            f'<Document href=\"{doc[\"url\"]}\"/>\\n{doc[\"content\"]}\\n</Document>'\n",
        "            for doc in search_docs\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return {\"context\": [formatted_search_docs]}\n",
        "\n",
        "def search_wikipedia(state: InterviewState):\n",
        "\n",
        "    \"\"\" Retrieve docs from wikipedia \"\"\"\n",
        "\n",
        "    # Search query\n",
        "    structured_llm = llm.with_structured_output(SearchQuery)\n",
        "    search_query = structured_llm.invoke([search_instructions]+state['messages'])\n",
        "\n",
        "    # Search\n",
        "    search_docs = WikipediaLoader(query=search_query.search_query,\n",
        "                                  load_max_docs=2).load()\n",
        "\n",
        "     # Format\n",
        "    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
        "        [\n",
        "            f'<Document source=\"{doc.metadata[\"source\"]}\" page=\"{doc.metadata.get(\"page\", \"\")}\"/>\\n{doc.page_content}\\n</Document>'\n",
        "            for doc in search_docs\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return {\"context\": [formatted_search_docs]}\n",
        "\n",
        "answer_instructions = \"\"\"You are an expert being interviewed by an analyst.\n",
        "\n",
        "Here is analyst area of focus: {goals}.\n",
        "\n",
        "You goal is to answer a question posed by the interviewer.\n",
        "\n",
        "To answer question, use this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "When answering questions, follow these guidelines:\n",
        "\n",
        "1. Use only the information provided in the context.\n",
        "\n",
        "2. Do not introduce external information or make assumptions beyond what is explicitly stated in the context.\n",
        "\n",
        "3. The context contain sources at the topic of each individual document.\n",
        "\n",
        "4. Include these sources your answer next to any relevant statements. For example, for source # 1 use [1].\n",
        "\n",
        "5. List your sources in order at the bottom of your answer. [1] Source 1, [2] Source 2, etc\n",
        "\n",
        "6. If the source is: <Document source=\"assistant/docs/llama3_1.pdf\" page=\"7\"/>' then just list:\n",
        "\n",
        "[1] assistant/docs/llama3_1.pdf, page 7\n",
        "\n",
        "And skip the addition of the brackets as well as the Document source preamble in your citation.\"\"\"\n",
        "\n",
        "def generate_answer(state: InterviewState):\n",
        "\n",
        "    \"\"\" Node to answer a question \"\"\"\n",
        "\n",
        "    # Get state\n",
        "    analyst = state[\"analyst\"]\n",
        "    messages = state[\"messages\"]\n",
        "    context = state[\"context\"]\n",
        "\n",
        "    # Answer question\n",
        "    system_message = answer_instructions.format(goals=analyst.persona, context=context)\n",
        "    answer = llm.invoke([SystemMessage(content=system_message)]+messages)\n",
        "\n",
        "    # Name the message as coming from the expert\n",
        "    answer.name = \"expert\"\n",
        "\n",
        "    # Append it to state\n",
        "    return {\"messages\": [answer]}\n",
        "\n",
        "def save_interview(state: InterviewState):\n",
        "\n",
        "    \"\"\" Save interviews \"\"\"\n",
        "\n",
        "    # Get messages\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    # Convert interview to a string\n",
        "    interview = get_buffer_string(messages)\n",
        "\n",
        "    # Save to interviews key\n",
        "    return {\"interview\": interview}\n",
        "\n",
        "def route_messages(state: InterviewState,\n",
        "                   name: str = \"expert\"):\n",
        "\n",
        "    \"\"\" Route between question and answer \"\"\"\n",
        "\n",
        "    # Get messages\n",
        "    messages = state[\"messages\"]\n",
        "    max_num_turns = state.get('max_num_turns',2)\n",
        "\n",
        "    # Check the number of expert answers\n",
        "    num_responses = len(\n",
        "        [m for m in messages if isinstance(m, AIMessage) and m.name == name]\n",
        "    )\n",
        "\n",
        "    # End if expert has answered more than the max turns\n",
        "    if num_responses >= max_num_turns:\n",
        "        return 'save_interview'\n",
        "\n",
        "    # This router is run after each question - answer pair\n",
        "    # Get the last question asked to check if it signals the end of discussion\n",
        "    last_question = messages[-2]\n",
        "\n",
        "    if \"Thank you so much for your help\" in last_question.content:\n",
        "        return 'save_interview'\n",
        "    return \"ask_question\"\n",
        "\n",
        "section_writer_instructions = \"\"\"You are an expert technical writer.\n",
        "\n",
        "Your task is to create a short, easily digestible section of a report based on a set of source documents.\n",
        "\n",
        "1. Analyze the content of the source documents:\n",
        "- The name of each source document is at the start of the document, with the <Document tag.\n",
        "\n",
        "2. Create a report structure using markdown formatting:\n",
        "- Use ## for the section title\n",
        "- Use ### for sub-section headers\n",
        "\n",
        "3. Write the report following this structure:\n",
        "a. Title (## header)\n",
        "b. Summary (### header)\n",
        "c. Sources (### header)\n",
        "\n",
        "4. Make your title engaging based upon the focus area of the analyst:\n",
        "{focus}\n",
        "\n",
        "5. For the summary section:\n",
        "- Set up summary with general background / context related to the focus area of the analyst\n",
        "- Emphasize what is novel, interesting, or surprising about insights gathered from the interview\n",
        "- Create a numbered list of source documents, as you use them\n",
        "- Do not mention the names of interviewers or experts\n",
        "- Aim for approximately 400 words maximum\n",
        "- Use numbered sources in your report (e.g., [1], [2]) based on information from source documents\n",
        "\n",
        "6. In the Sources section:\n",
        "- Include all sources used in your report\n",
        "- Provide full links to relevant websites or specific document paths\n",
        "- Separate each source by a newline. Use two spaces at the end of each line to create a newline in Markdown.\n",
        "- It will look like:\n",
        "\n",
        "### Sources\n",
        "[1] Link or Document name\n",
        "[2] Link or Document name\n",
        "\n",
        "7. Be sure to combine sources. For example this is not correct:\n",
        "\n",
        "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
        "[4] https://ai.meta.com/blog/meta-llama-3-1/\n",
        "\n",
        "There should be no redundant sources. It should simply be:\n",
        "\n",
        "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
        "\n",
        "8. Final review:\n",
        "- Ensure the report follows the required structure\n",
        "- Include no preamble before the title of the report\n",
        "- Check that all guidelines have been followed\"\"\"\n",
        "\n",
        "def write_section(state: InterviewState):\n",
        "\n",
        "    \"\"\" Node to answer a question \"\"\"\n",
        "\n",
        "    # Get state\n",
        "    interview = state[\"interview\"]\n",
        "    context = state[\"context\"]\n",
        "    analyst = state[\"analyst\"]\n",
        "\n",
        "    # Write section using either the gathered source docs from interview (context) or the interview itself (interview)\n",
        "    system_message = section_writer_instructions.format(focus=analyst.description)\n",
        "    section = llm.invoke([SystemMessage(content=system_message)]+[HumanMessage(content=f\"Use this source to write your section: {context}\")])\n",
        "\n",
        "    # Append it to state\n",
        "    return {\"sections\": [section.content]}\n",
        "\n",
        "# Add nodes and edges\n",
        "interview_builder = StateGraph(InterviewState)\n",
        "interview_builder.add_node(\"ask_question\", generate_question)\n",
        "interview_builder.add_node(\"search_web\", search_web)\n",
        "interview_builder.add_node(\"search_wikipedia\", search_wikipedia)\n",
        "interview_builder.add_node(\"answer_question\", generate_answer)\n",
        "interview_builder.add_node(\"save_interview\", save_interview)\n",
        "interview_builder.add_node(\"write_section\", write_section)\n",
        "\n",
        "# Flow\n",
        "interview_builder.add_edge(START, \"ask_question\")\n",
        "interview_builder.add_edge(\"ask_question\", \"search_web\")\n",
        "interview_builder.add_edge(\"ask_question\", \"search_wikipedia\")\n",
        "interview_builder.add_edge(\"search_web\", \"answer_question\")\n",
        "interview_builder.add_edge(\"search_wikipedia\", \"answer_question\")\n",
        "interview_builder.add_conditional_edges(\"answer_question\", route_messages,['ask_question','save_interview'])\n",
        "interview_builder.add_edge(\"save_interview\", \"write_section\")\n",
        "interview_builder.add_edge(\"write_section\", END)\n",
        "\n",
        "# Interview\n",
        "memory = MemorySaver()\n",
        "interview_graph = interview_builder.compile(checkpointer=memory).with_config(run_name=\"Conduct Interviews\")\n",
        "\n",
        "# View\n",
        "display(Image(interview_graph.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PqpcNTrpjmA6",
      "metadata": {
        "id": "PqpcNTrpjmA6"
      },
      "outputs": [],
      "source": [
        "from langgraph.constants import Send\n",
        "\n",
        "def initiate_all_interviews(state: ResearchGraphState):\n",
        "    \"\"\" This is the \"map\" step where we run each interview sub-graph using Send API \"\"\"\n",
        "\n",
        "    # Check if human feedback\n",
        "    human_analyst_feedback=state.get('human_analyst_feedback')\n",
        "    if human_analyst_feedback:\n",
        "        # Return to create_analysts\n",
        "        return \"create_analysts\"\n",
        "\n",
        "    # Otherwise kick off interviews in parallel via Send() API\n",
        "    else:\n",
        "        topic = state[\"topic\"]\n",
        "        return [Send(\"conduct_interview\", {\"analyst\": analyst,\n",
        "                                           \"messages\": [HumanMessage(\n",
        "                                               content=f\"So you said you were writing an article on {topic}?\"\n",
        "                                           )\n",
        "                                                       ]}) for analyst in state[\"analysts\"]]\n",
        "\n",
        "report_writer_instructions = \"\"\"You are a technical writer creating a report on this overall topic:\n",
        "\n",
        "{topic}\n",
        "\n",
        "You have a team of analysts. Each analyst has done two things:\n",
        "\n",
        "1. They conducted an interview with an expert on a specific sub-topic.\n",
        "2. They write up their finding into a memo.\n",
        "\n",
        "Your task:\n",
        "\n",
        "1. You will be given a collection of memos from your analysts.\n",
        "2. Think carefully about the insights from each memo.\n",
        "3. Consolidate these into a crisp overall summary that ties together the central ideas from all of the memos.\n",
        "4. Summarize the central points in each memo into a cohesive single narrative.\n",
        "\n",
        "To format your report:\n",
        "\n",
        "1. Use markdown formatting.\n",
        "2. Include no pre-amble for the report.\n",
        "3. Use no sub-heading.\n",
        "4. Start your report with a single title header: ## Insights\n",
        "5. Do not mention any analyst names in your report.\n",
        "6. Preserve any citations in the memos, which will be annotated in brackets, for example [1] or [2].\n",
        "7. Create a final, consolidated list of sources and add to a Sources section with the `## Sources` header.\n",
        "8. List your sources in order and do not repeat.\n",
        "\n",
        "[1] Source 1\n",
        "[2] Source 2\n",
        "\n",
        "Here are the memos from your analysts to build your report from:\n",
        "\n",
        "{context}\"\"\"\n",
        "\n",
        "def write_report(state: ResearchGraphState):\n",
        "    # Full set of sections\n",
        "    sections = state[\"sections\"]\n",
        "    topic = state[\"topic\"]\n",
        "\n",
        "    # Concat all sections together\n",
        "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
        "\n",
        "    # Summarize the sections into a final report\n",
        "    system_message = report_writer_instructions.format(topic=topic, context=formatted_str_sections)\n",
        "    report = llm.invoke([SystemMessage(content=system_message)]+[HumanMessage(content=f\"Write a report based upon these memos.\")])\n",
        "    return {\"content\": report.content}\n",
        "\n",
        "intro_conclusion_instructions = \"\"\"You are a technical writer finishing a report on {topic}\n",
        "\n",
        "You will be given all of the sections of the report.\n",
        "\n",
        "You job is to write a crisp and compelling introduction or conclusion section.\n",
        "\n",
        "The user will instruct you whether to write the introduction or conclusion.\n",
        "\n",
        "Include no pre-amble for either section.\n",
        "\n",
        "Target around 100 words, crisply previewing (for introduction) or recapping (for conclusion) all of the sections of the report.\n",
        "\n",
        "Use markdown formatting.\n",
        "\n",
        "For your introduction, create a compelling title and use the # header for the title.\n",
        "\n",
        "For your introduction, use ## Introduction as the section header.\n",
        "\n",
        "For your conclusion, use ## Conclusion as the section header.\n",
        "\n",
        "Here are the sections to reflect on for writing: {formatted_str_sections}\"\"\"\n",
        "\n",
        "def write_introduction(state: ResearchGraphState):\n",
        "    # Full set of sections\n",
        "    sections = state[\"sections\"]\n",
        "    topic = state[\"topic\"]\n",
        "\n",
        "    # Concat all sections together\n",
        "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
        "\n",
        "    # Summarize the sections into a final report\n",
        "\n",
        "    instructions = intro_conclusion_instructions.format(topic=topic, formatted_str_sections=formatted_str_sections)\n",
        "    intro = llm.invoke([instructions]+[HumanMessage(content=f\"Write the report introduction\")])\n",
        "    return {\"introduction\": intro.content}\n",
        "\n",
        "def write_conclusion(state: ResearchGraphState):\n",
        "    # Full set of sections\n",
        "    sections = state[\"sections\"]\n",
        "    topic = state[\"topic\"]\n",
        "\n",
        "    # Concat all sections together\n",
        "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
        "\n",
        "    # Summarize the sections into a final report\n",
        "\n",
        "    instructions = intro_conclusion_instructions.format(topic=topic, formatted_str_sections=formatted_str_sections)\n",
        "    conclusion = llm.invoke([instructions]+[HumanMessage(content=f\"Write the report conclusion\")])\n",
        "    return {\"conclusion\": conclusion.content}\n",
        "\n",
        "def finalize_report(state: ResearchGraphState):\n",
        "    \"\"\" The is the \"reduce\" step where we gather all the sections, combine them, and reflect on them to write the intro/conclusion \"\"\"\n",
        "    # Save full final report\n",
        "    content = state[\"content\"]\n",
        "    if content.startswith(\"## Insights\"):\n",
        "        content = content.strip(\"## Insights\")\n",
        "    if \"## Sources\" in content:\n",
        "        try:\n",
        "            content, sources = content.split(\"\\n## Sources\\n\")\n",
        "        except:\n",
        "            sources = None\n",
        "    else:\n",
        "        sources = None\n",
        "\n",
        "    final_report = state[\"introduction\"] + \"\\n\\n---\\n\\n\" + content + \"\\n\\n---\\n\\n\" + state[\"conclusion\"]\n",
        "    if sources is not None:\n",
        "        final_report += \"\\n\\n## Sources\\n\" + sources\n",
        "    return {\"final_report\": final_report}\n",
        "\n",
        "# Add nodes and edges\n",
        "builder = StateGraph(ResearchGraphState)\n",
        "builder.add_node(\"create_analysts\", create_analysts)\n",
        "builder.add_node(\"human_feedback\", human_feedback)\n",
        "builder.add_node(\"conduct_interview\", interview_builder.compile())\n",
        "builder.add_node(\"write_introduction\",write_introduction)\n",
        "builder.add_node(\"write_conclusion\",write_conclusion)\n",
        "builder.add_node(\"write_report\",write_report)\n",
        "builder.add_node(\"finalize_report\",finalize_report)\n",
        "\n",
        "# Logic\n",
        "builder.add_edge(START, \"create_analysts\")\n",
        "builder.add_edge(\"create_analysts\", \"human_feedback\")\n",
        "builder.add_conditional_edges(\"human_feedback\", initiate_all_interviews, [\"create_analysts\", \"conduct_interview\"])\n",
        "builder.add_edge(\"conduct_interview\", \"write_introduction\")\n",
        "builder.add_edge(\"conduct_interview\", \"write_conclusion\")\n",
        "builder.add_edge(\"conduct_interview\", \"write_report\")\n",
        "builder.add_edge([\"write_conclusion\", \"write_report\", \"write_introduction\"], \"finalize_report\")\n",
        "builder.add_edge(\"finalize_report\", END)\n",
        "\n",
        "# Compile\n",
        "memory = MemorySaver()\n",
        "graph = builder.compile(interrupt_before=['human_feedback'], checkpointer=memory)\n",
        "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CQRq8KSiXBxZ",
      "metadata": {
        "id": "CQRq8KSiXBxZ"
      },
      "source": [
        "# 프롬프트 최적화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_95A18LPXDMO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 2336,
          "status": "ok",
          "timestamp": 1728869416144,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "_95A18LPXDMO",
        "outputId": "bc28a507-274f-4fe9-f73b-34a164d47df8"
      },
      "outputs": [],
      "source": [
        "# 컬럼명 & 컬럼용 질문 Dict 생성\n",
        "qq = singer_queries['member_list']['question']\n",
        "loc = '멤버'\n",
        "\n",
        "# 시스템 프롬프트 설정\n",
        "system_prompt = SystemMessagePromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    You are a biographical dictionary agent. Provide only factual information based solely on the provided Context.\n",
        "    Be friendly and helpful without being overly chatty. Do not use any prior knowledge beyond the Context.\n",
        "    If the answer is not found in the Context, respond with \"None\" Follow the specified Output Format below.\n",
        "\n",
        "    The input will be provided using the following tags:\n",
        "    <Context>...</Context>\n",
        "    <Question>...</Question>\n",
        "\n",
        "    Please extract the Context, Question, and Format from these tags and answer the query accordingly.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "human_prompt = HumanMessagePromptTemplate.from_template(\n",
        "    \"\"\"<Context>n{context}</Context>\\n\\n\n",
        "    <Question>{question}</Question>\\n\\n\n",
        "    \"\"\"\n",
        ")\n",
        "prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
        "chain = {\n",
        "      'context': itemgetter('context') | RunnablePassthrough()\n",
        "    , 'question': itemgetter('question') | RunnablePassthrough()\n",
        "} | prompt | llm | StrOutputParser()\n",
        "\n",
        "result = chain.with_config(configurable={\n",
        "            #   \"llm\": 'gpt4o_mini',\n",
        "            \"llm\":\"gemini_flash\",\n",
        "              \"temparature\": 0,\n",
        "              \"max_tokens\": None #max_tokens\n",
        "          }).invoke({\"question\": qq, 'context': \"\\n\\n\".join([doc.page_content for doc in documents if doc.metadata['abs_page_toc_item'] == loc])})\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tzrRXy2ipdAI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1728869423989,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "tzrRXy2ipdAI",
        "outputId": "6af5e8b7-381b-43a8-c923-0f65dcf24678"
      },
      "outputs": [],
      "source": [
        "type(chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CeJtAxVBX_29",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "executionInfo": {
          "elapsed": 308,
          "status": "ok",
          "timestamp": 1727741146613,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "CeJtAxVBX_29",
        "outputId": "1ad9b47f-d05a-4100-cfbd-6a2b4aa99ded"
      },
      "outputs": [],
      "source": [
        "qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0D6SuYEHXvMm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "executionInfo": {
          "elapsed": 8,
          "status": "ok",
          "timestamp": 1727742503065,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "0D6SuYEHXvMm",
        "outputId": "a53cd354-da67-402c-f9d3-e3de87e6627d"
      },
      "outputs": [],
      "source": [
        "\"\\n\\n\".join([doc.page_content for doc in documents if doc.metadata['parent_paper'] == '팬덤'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "art_meta_parse_agent",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0rc2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
