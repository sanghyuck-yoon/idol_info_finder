{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # .env 파일에서 환경 변수를 불러옵니다.\n",
    "\n",
    "# api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# print(f\"API Key: {api_key}\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "search_engine_key = os.getenv(\"GOOGLE_SEARCH_ENGINE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   we_art_id we_art_name\n",
      "0          1         TXT\n",
      "1          2         BTS\n",
      "2          3     GFRIEND\n",
      "3          4   SEVENTEEN\n",
      "4          5     ENHYPEN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 파일 경로\n",
    "file_path = 'C:\\\\Users\\\\sanghyoon\\\\Desktop\\\\we_artist.csv'\n",
    "\n",
    "# CSV 파일 읽기 (ISO-8859-1 인코딩 사용)\n",
    "we_artist = pd.read_csv(file_path, encoding='UTF-8')\n",
    "\n",
    "# 읽은 데이터 출력\n",
    "print(we_artist.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-bigquery in c:\\python\\lib\\site-packages (from -r requirements.txt (line 1)) (3.25.0)\n",
      "Requirement already satisfied: openai in c:\\python\\lib\\site-packages (from -r requirements.txt (line 2)) (1.37.1)\n",
      "Requirement already satisfied: langchain-community in c:\\python\\lib\\site-packages (from -r requirements.txt (line 3)) (0.2.7)\n",
      "Requirement already satisfied: langchain-google-vertexai in c:\\python\\lib\\site-packages (from -r requirements.txt (line 4)) (1.0.7)\n",
      "Requirement already satisfied: langchain-google-community in c:\\python\\lib\\site-packages (from -r requirements.txt (line 5)) (1.0.7)\n",
      "Requirement already satisfied: requests in c:\\python\\lib\\site-packages (from -r requirements.txt (line 6)) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\python\\lib\\site-packages (from -r requirements.txt (line 7)) (4.12.3)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in c:\\python\\lib\\site-packages (from google-cloud-bigquery->-r requirements.txt (line 1)) (2.32.0)\n",
      "Requirement already satisfied: packaging>=20.0.0 in c:\\users\\sanghyoon\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-bigquery->-r requirements.txt (line 1)) (24.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in c:\\python\\lib\\site-packages (from google-cloud-bigquery->-r requirements.txt (line 1)) (2.4.1)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in c:\\python\\lib\\site-packages (from google-cloud-bigquery->-r requirements.txt (line 1)) (2.19.1)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in c:\\users\\sanghyoon\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-bigquery->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in c:\\python\\lib\\site-packages (from google-cloud-bigquery->-r requirements.txt (line 1)) (2.7.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\python\\lib\\site-packages (from openai->-r requirements.txt (line 2)) (0.27.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\sanghyoon\\appdata\\roaming\\python\\python310\\site-packages (from openai->-r requirements.txt (line 2)) (4.12.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\python\\lib\\site-packages (from openai->-r requirements.txt (line 2)) (1.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\python\\lib\\site-packages (from openai->-r requirements.txt (line 2)) (4.4.0)\n",
      "Requirement already satisfied: sniffio in c:\\python\\lib\\site-packages (from openai->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\python\\lib\\site-packages (from openai->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: tqdm>4 in c:\\python\\lib\\site-packages (from openai->-r requirements.txt (line 2)) (4.66.4)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.7 in c:\\python\\lib\\site-packages (from langchain-community->-r requirements.txt (line 3)) (0.2.7)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\python\\lib\\site-packages (from langchain-community->-r requirements.txt (line 3)) (0.1.84)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\python\\lib\\site-packages (from langchain-community->-r requirements.txt (line 3)) (6.0.1)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.12 in c:\\python\\lib\\site-packages (from langchain-community->-r requirements.txt (line 3)) (0.2.24)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\python\\lib\\site-packages (from langchain-community->-r requirements.txt (line 3)) (0.6.7)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\python\\lib\\site-packages (from langchain-community->-r requirements.txt (line 3)) (3.9.5)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\python\\lib\\site-packages (from langchain-community->-r requirements.txt (line 3)) (1.26.4)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\python\\lib\\site-packages (from langchain-community->-r requirements.txt (line 3)) (8.3.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\python\\lib\\site-packages (from langchain-community->-r requirements.txt (line 3)) (2.0.31)\n",
      "Requirement already satisfied: google-cloud-aiplatform<2.0.0,>=1.56.0 in c:\\python\\lib\\site-packages (from langchain-google-vertexai->-r requirements.txt (line 4)) (1.60.0)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0,>=2.17.0 in c:\\python\\lib\\site-packages (from langchain-google-vertexai->-r requirements.txt (line 4)) (2.18.0)\n",
      "Requirement already satisfied: google-api-python-client<3.0.0,>=2.122.0 in c:\\python\\lib\\site-packages (from langchain-google-community->-r requirements.txt (line 5)) (2.137.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.62.0 in c:\\python\\lib\\site-packages (from langchain-google-community->-r requirements.txt (line 5)) (1.65.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\lib\\site-packages (from requests->-r requirements.txt (line 6)) (2024.7.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python\\lib\\site-packages (from requests->-r requirements.txt (line 6)) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python\\lib\\site-packages (from requests->-r requirements.txt (line 6)) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python\\lib\\site-packages (from requests->-r requirements.txt (line 6)) (2.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\python\\lib\\site-packages (from beautifulsoup4->-r requirements.txt (line 7)) (2.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\python\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\python\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 3)) (6.0.5)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\python\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 3)) (4.0.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\python\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 3)) (1.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\python\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 3)) (23.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\python\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 3)) (1.9.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\sanghyoon\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 2)) (1.2.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\python\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 3)) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\python\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 3)) (3.21.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\python\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery->-r requirements.txt (line 1)) (1.24.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\python\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery->-r requirements.txt (line 1)) (1.63.2)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in c:\\python\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery->-r requirements.txt (line 1)) (4.25.4)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\python\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery->-r requirements.txt (line 1)) (1.62.2)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in c:\\python\\lib\\site-packages (from google-api-python-client<3.0.0,>=2.122.0->langchain-google-community->-r requirements.txt (line 5)) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\python\\lib\\site-packages (from google-api-python-client<3.0.0,>=2.122.0->langchain-google-community->-r requirements.txt (line 5)) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\python\\lib\\site-packages (from google-api-python-client<3.0.0,>=2.122.0->langchain-google-community->-r requirements.txt (line 5)) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\python\\lib\\site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery->-r requirements.txt (line 1)) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\python\\lib\\site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery->-r requirements.txt (line 1)) (5.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\python\\lib\\site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery->-r requirements.txt (line 1)) (0.4.0)\n",
      "Requirement already satisfied: shapely<3.0.0dev in c:\\python\\lib\\site-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai->-r requirements.txt (line 4)) (2.0.5)\n",
      "Requirement already satisfied: docstring-parser<1 in c:\\python\\lib\\site-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai->-r requirements.txt (line 4)) (0.16)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in c:\\python\\lib\\site-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai->-r requirements.txt (line 4)) (1.12.4)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in c:\\python\\lib\\site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai->-r requirements.txt (line 4)) (0.13.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\python\\lib\\site-packages (from google-cloud-storage<3.0.0,>=2.17.0->langchain-google-vertexai->-r requirements.txt (line 4)) (1.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\python\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client<3.0.0,>=2.122.0->langchain-google-community->-r requirements.txt (line 5)) (3.1.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\python\\lib\\site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 2)) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\python\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 2)) (0.14.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\python\\lib\\site-packages (from langchain<0.3.0,>=0.2.7->langchain-community->-r requirements.txt (line 3)) (0.2.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\python\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.12->langchain-community->-r requirements.txt (line 3)) (1.33)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\python\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.12->langchain-community->-r requirements.txt (line 3)) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\python\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community->-r requirements.txt (line 3)) (3.10.6)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\python\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery->-r requirements.txt (line 1)) (0.6.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\python\\lib\\site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\python\\lib\\site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 2)) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sanghyoon\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\python\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community->-r requirements.txt (line 3)) (3.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\sanghyoon\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>4->openai->-r requirements.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\python\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 3)) (1.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Python\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 TXT\n",
      "https://namu.wiki/w/투모로우바이투게더\n",
      "1 2 BTS\n",
      "https://namu.wiki/w/방탄소년단\n",
      "2 3 GFRIEND\n",
      "https://namu.wiki/w/여자친구(아이돌)\n",
      "3 4 SEVENTEEN\n",
      "https://namu.wiki/w/세븐틴\n",
      "4 5 ENHYPEN\n",
      "https://namu.wiki/w/ENHYPEN\n",
      "5 6 NU'EST\n",
      "https://namu.wiki/w/뉴이스트\n",
      "6 7 CL\n",
      "https://namu.wiki/w/CL\n",
      "7 8 P1HARMONY\n",
      "https://namu.wiki/w/P1Harmony\n",
      "8 9 WEEEKLY\n",
      "https://namu.wiki/w/Weeekly\n",
      "9 10 SUNMI\n",
      "https://namu.wiki/w/선미\n",
      "10 12 HENRY\n",
      "https://namu.wiki/w/헨리(가수)\n",
      "11 13 DREAMCATCHER\n",
      "https://namu.wiki/w/드림캐쳐(아이돌)\n",
      "12 14 2022 WEVERSE CON\n",
      "https://namu.wiki/w/2022%20Weverse%20Con\n",
      "13 15 GRACIE ABRAMS\n",
      "https://namu.wiki/w/그레이시%20에이브럼스\n",
      "14 16 CHERRY BULLET\n",
      "https://namu.wiki/w/체리블렛\n",
      "15 17 NEW HOPE CLUB\n",
      "https://namu.wiki/w/New%20Hope%20Club\n",
      "16 18 ALEXANDER 23\n",
      "https://namu.wiki/w/트렌트%20알렉산더아놀드\n",
      "17 19 MIRAE\n",
      "https://namu.wiki/w/미래소년\n",
      "18 20 TREASURE\n",
      "https://namu.wiki/w/TREASURE\n",
      "19 21 HYBE INSIGHT\n",
      "https://namu.wiki/w/HYBE%20LABELS\n",
      "20 22 LETTEAMOR\n",
      "https://namu.wiki/w/레떼아모르\n",
      "21 23 JEREMY ZUCKER\n",
      "https://namu.wiki/w/제레미%20주커\n",
      "22 24 PRETTYMUCH\n",
      "https://namu.wiki/w/Prettymuch\n",
      "23 25 WOOAH\n",
      "https://namu.wiki/w/WOOAH\n",
      "24 26 MAX\n",
      "https://namu.wiki/w/M.C%20The%20Max\n",
      "25 27 FTISLAND\n",
      "https://namu.wiki/w/FTISLAND\n",
      "26 28 EVERGLOW\n",
      "https://namu.wiki/w/EVERGLOW\n",
      "27 29 IKON\n",
      "https://namu.wiki/w/iKON\n",
      "28 30 JUST B\n",
      "https://namu.wiki/w/JUST%20B\n",
      "29 31 MAD MONSTER\n",
      "https://namu.wiki/w/매드몬스터\n",
      "30 32 BLACKPINK\n",
      "https://namu.wiki/w/BLACKPINK\n",
      "31 33 STAYC\n",
      "https://namu.wiki/w/STAYC\n",
      "32 34 LILHUDDY\n",
      "LILHUDDY\n",
      "Can't Not found Urls from keyword\n",
      "N/A\n",
      "33 35 PURPLE KISS\n",
      "https://namu.wiki/w/퍼플키스\n",
      "34 36 FROMIS_9\n",
      "https://namu.wiki/w/프로미스나인\n",
      "35 37 PRIKIL\n",
      "https://namu.wiki/w/PRIKIL\n",
      "36 38 WINNER\n",
      "https://namu.wiki/w/WINNER\n",
      "37 39 ONEUS\n",
      "https://namu.wiki/w/원어스\n",
      "38 40 RAVI\n",
      "https://namu.wiki/w/라비(가수)\n",
      "39 41 KIMJUNSU\n",
      "https://namu.wiki/w/김준수(JYJ)\n",
      "40 42 VERIVERY\n",
      "https://namu.wiki/w/VERIVERY\n",
      "41 43 UP10TION\n",
      "https://namu.wiki/w/업텐션\n",
      "42 44 이진혁 (종료)\n",
      "https://namu.wiki/w/이진혁(1996)\n",
      "43 45 XG\n",
      "https://namu.wiki/w/XG\n",
      "44 46 LE SSERAFIM\n",
      "https://namu.wiki/w/LE%20SSERAFIM\n",
      "45 47 BLITZERS\n",
      "https://namu.wiki/w/블리처스\n",
      "46 48 THE KINGDOM\n",
      "https://namu.wiki/w/The%20KingDom\n",
      "47 49 YOON JISUNG\n",
      "https://namu.wiki/w/윤지성\n",
      "48 50 HWANG MIN HYUN\n",
      "https://namu.wiki/w/황민현\n",
      "49 51 BAEKHO(KANG DONG HO)\n",
      "https://namu.wiki/w/백호(가수)\n",
      "50 52 APINK\n",
      "https://namu.wiki/w/Apink\n",
      "51 53 VICTON\n",
      "https://namu.wiki/w/VICTON\n",
      "52 54 THE NEW SIX\n",
      "https://namu.wiki/w/THE%20NEW%20SIX\n",
      "53 55 SECRET NUMBER\n",
      "https://namu.wiki/w/SECRET%20NUMBER\n",
      "54 56 &TEAM\n",
      "https://namu.wiki/w/%26TEAM\n",
      "55 57 TRI.BE\n",
      "https://namu.wiki/w/TRI.BE%20Da%20Loca\n",
      "56 58 ONLYONEOF\n",
      "https://namu.wiki/w/OnlyOneOf\n",
      "57 59 ROCKET PUNCH\n",
      "https://namu.wiki/w/로켓펀치\n",
      "58 60 HYOLYN\n",
      "https://namu.wiki/w/효린\n",
      "59 61 ATBO\n",
      "https://namu.wiki/w/ATBO\n",
      "60 62 GOLDEN CHILD\n",
      "https://namu.wiki/w/골든차일드\n",
      "61 64 ZICO\n",
      "https://namu.wiki/w/지코\n",
      "62 69 NEWJEANS_PHONING\n",
      "https://namu.wiki/w/NewJeans/Phoning%20LIVE\n",
      "63 71 TFN\n",
      "https://namu.wiki/w/TFN\n",
      "64 72 OH MY GIRL\n",
      "https://namu.wiki/w/오마이걸\n",
      "65 73 CNBLUE\n",
      "https://namu.wiki/w/CNBLUE\n",
      "66 74 BILLLIE\n",
      "https://namu.wiki/w/Billlie\n",
      "67 75 B.I.G\n",
      "https://namu.wiki/w/비아이지\n",
      "68 76 WEVERSE ZONE\n",
      "https://namu.wiki/w/The%20Debut:%20Dream%20Academy?rev=94\n",
      "69 77 3YE\n",
      "https://namu.wiki/w/3YE\n",
      "70 78 AKMU\n",
      "https://namu.wiki/w/AKMU\n",
      "71 79 MINZY\n",
      "https://namu.wiki/w/공민지\n",
      "72 80 KIM WOO SEOK\n",
      "https://namu.wiki/w/김우석(1996)/프로듀스%20X%20101\n",
      "73 81 NEWJEANS\n",
      "https://namu.wiki/w/NewJeans\n",
      "74 82 WHEE IN\n",
      "https://namu.wiki/w/휘인\n",
      "75 83 B1A4\n",
      "https://namu.wiki/w/B1A4\n",
      "76 84 PARK BO YOUNG\n",
      "https://namu.wiki/w/박보영\n",
      "77 85 LEEHI\n",
      "https://namu.wiki/w/이하이\n",
      "78 86 BAMBAM\n",
      "https://namu.wiki/w/뱀뱀\n",
      "79 87 ONF\n",
      "https://namu.wiki/w/온앤오프\n",
      "80 88 BIGBANG\n",
      "https://namu.wiki/w/BIGBANG\n",
      "81 89 KIM SEJEONG\n",
      "https://namu.wiki/w/김세정\n",
      "82 90 VIXX\n",
      "https://namu.wiki/w/VIXX\n",
      "83 91 THE BOYZ\n",
      "https://namu.wiki/w/더보이즈\n",
      "84 92 KIM SEON HO\n",
      "https://namu.wiki/w/김선호\n",
      "85 93 E'LAST\n",
      "https://namu.wiki/w/엘라스트\n",
      "86 94 YURINA HIRATE\n",
      "https://namu.wiki/w/히라테%20유리나\n",
      "87 95 DRIPPIN\n",
      "https://namu.wiki/w/Boyager\n",
      "88 96 PENTAGON\n",
      "https://namu.wiki/w/펜타곤(아이돌)\n",
      "89 97 BTOB\n",
      "https://namu.wiki/w/비투비\n",
      "90 98 MOONBIN&SANHA\n",
      "https://namu.wiki/w/문빈%26산하\n",
      "91 99 KWON EUN BI\n",
      "https://namu.wiki/w/권은비\n",
      "92 100 (G)I-DLE\n",
      "https://namu.wiki/w/(여자)아이들\n",
      "93 101 AKB48\n",
      "https://namu.wiki/w/AKB48\n",
      "94 102 BOYNEXTDOOR\n",
      "https://namu.wiki/w/BOYNEXTDOOR\n",
      "95 103 CLASS:Y\n",
      "https://namu.wiki/w/CLASS:y\n",
      "96 104 MOONCHILD\n",
      "https://namu.wiki/w/MOONCHILD\n",
      "97 105 nan\n",
      "https://namu.wiki/w/NUN%20NU%20NAN%20NA\n",
      "98 106 LIGHTSUM\n",
      "https://namu.wiki/w/LIGHTSUM\n",
      "99 107 LEE SOO HYUK\n",
      "https://namu.wiki/w/이수혁\n",
      "100 108 LEE JIN HYUK\n",
      "https://namu.wiki/w/이진혁(1996)\n",
      "101 109 IMASE\n",
      "https://namu.wiki/w/imase\n",
      "102 110 SON NA EUN\n",
      "https://namu.wiki/w/손나은\n",
      "103 111 ILLIT\n",
      "https://namu.wiki/w/ILLIT\n",
      "104 112 LUN8\n",
      "https://namu.wiki/w/LUN8\n",
      "105 113 KIM JAE JOONG\n",
      "https://namu.wiki/w/김재중\n",
      "106 114 CHOI SOO HO\n"
     ]
    },
    {
     "ename": "HttpError",
     "evalue": "<HttpError 429 when requesting https://customsearch.googleapis.com/customsearch/v1?q=CHOI+SOO+HO&cx=147945577870949cd&siteSearch=https%3A%2F%2Fnamu.wiki%2Fw%2F&siteSearchFilter=i&cr=countryKR&hl=ko&orTerms=%EB%8D%B0%EB%B7%94&key=AIzaSyCdtmlzJyXQe6ep1UBWnGIKdNlDJho8bpo&alt=json returned \"Quota exceeded for quota metric 'Queries' and limit 'Queries per day' of service 'customsearch.googleapis.com' for consumer 'project_number:853670440911'.\". Details: \"[{'message': \"Quota exceeded for quota metric 'Queries' and limit 'Queries per day' of service 'customsearch.googleapis.com' for consumer 'project_number:853670440911'.\", 'domain': 'global', 'reason': 'rateLimitExceeded'}]\">",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m we_artist\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(idx, row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwe_art_id\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwe_art_name\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 11\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[43minst1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwe_art_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(url)\n\u001b[0;32m     13\u001b[0m     we_artist2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([we_artist2, pd\u001b[38;5;241m.\u001b[39mDataFrame([[row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwe_art_id\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwe_art_name\u001b[39m\u001b[38;5;124m'\u001b[39m], url]], columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwe_art_id\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwe_art_name\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m])])\n",
      "File \u001b[1;32mc:\\Users\\sanghyoon\\Documents\\Github\\idol_info_finder\\idol_info_finder-2\\get_namu_url.py:33\u001b[0m, in \u001b[0;36mGetNamuUrl.get_url\u001b[1;34m(self, query, siteDomain)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m페이지 URL 반환\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     17\u001b[0m service \u001b[38;5;241m=\u001b[39m build(\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustomsearch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1\u001b[39m\u001b[38;5;124m\"\u001b[39m, developerKey\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoogle_api_key \n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     21\u001b[0m res \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     22\u001b[0m \u001b[43mservice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_engine_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43msiteSearch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msiteDomain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43msiteSearchFilter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcountryKR\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mko\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43morTerms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m데뷔\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m---> 33\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m )\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m     pprint\u001b[38;5;241m.\u001b[39mpprint(res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearchInformation\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\googleapiclient\\_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[0;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\googleapiclient\\http.py:938\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[1;34m(self, http, num_retries)\u001b[0m\n\u001b[0;32m    936\u001b[0m     callback(resp)\n\u001b[0;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpError(resp, content, uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muri)\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostproc(resp, content)\n",
      "\u001b[1;31mHttpError\u001b[0m: <HttpError 429 when requesting https://customsearch.googleapis.com/customsearch/v1?q=CHOI+SOO+HO&cx=147945577870949cd&siteSearch=https%3A%2F%2Fnamu.wiki%2Fw%2F&siteSearchFilter=i&cr=countryKR&hl=ko&orTerms=%EB%8D%B0%EB%B7%94&key=AIzaSyCdtmlzJyXQe6ep1UBWnGIKdNlDJho8bpo&alt=json returned \"Quota exceeded for quota metric 'Queries' and limit 'Queries per day' of service 'customsearch.googleapis.com' for consumer 'project_number:853670440911'.\". Details: \"[{'message': \"Quota exceeded for quota metric 'Queries' and limit 'Queries per day' of service 'customsearch.googleapis.com' for consumer 'project_number:853670440911'.\", 'domain': 'global', 'reason': 'rateLimitExceeded'}]\">"
     ]
    }
   ],
   "source": [
    "from get_namu_url import GetNamuUrl\n",
    "\n",
    "inst1 = GetNamuUrl(google_api_key, search_engine_key)\n",
    "\n",
    "# we_artist['url_link'] = we_artist.apply(lambda x:inst1.get_url(x['we_art_name']), axis = 1)\n",
    "\n",
    "we_artist2 = pd.DataFrame(columns=['we_art_id','we_art_name','url'])\n",
    "\n",
    "for idx, row in we_artist.iterrows():\n",
    "    print(idx, row['we_art_id'], row['we_art_name'])\n",
    "    url = inst1.get_url(row['we_art_name'])\n",
    "    print(url)\n",
    "    we_artist2 = pd.concat([we_artist2, pd.DataFrame([[row['we_art_id'], row['we_art_name'], url]], columns=['we_art_id','we_art_name','url'])])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting delta-sharing\n",
      "  Downloading delta_sharing-1.0.5-py3-none-any.whl (17 kB)\n",
      "Collecting pyarrow>=4.0.0\n",
      "  Downloading pyarrow-17.0.0-cp310-cp310-win_amd64.whl (25.1 MB)\n",
      "Collecting fsspec>=0.7.4\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\python\\lib\\site-packages (from delta-sharing) (3.9.5)\n",
      "Requirement already satisfied: requests in c:\\python\\lib\\site-packages (from delta-sharing) (2.32.3)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.2-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "Requirement already satisfied: yarl>=1.6.0 in c:\\python\\lib\\site-packages (from delta-sharing) (1.9.4)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\python\\lib\\site-packages (from pyarrow>=4.0.0->delta-sharing) (1.26.4)\n",
      "Requirement already satisfied: multidict>=4.0 in c:\\python\\lib\\site-packages (from yarl>=1.6.0->delta-sharing) (6.0.5)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\python\\lib\\site-packages (from yarl>=1.6.0->delta-sharing) (3.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\python\\lib\\site-packages (from aiohttp->delta-sharing) (23.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\python\\lib\\site-packages (from aiohttp->delta-sharing) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\python\\lib\\site-packages (from aiohttp->delta-sharing) (4.0.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\python\\lib\\site-packages (from aiohttp->delta-sharing) (1.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sanghyoon\\appdata\\roaming\\python\\python310\\site-packages (from pandas->delta-sharing) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sanghyoon\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas->delta-sharing) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python\\lib\\site-packages (from requests->delta-sharing) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python\\lib\\site-packages (from requests->delta-sharing) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\lib\\site-packages (from requests->delta-sharing) (2024.7.4)\n",
      "Installing collected packages: tzdata, pytz, pyarrow, pandas, fsspec, delta-sharing\n",
      "Successfully installed delta-sharing-1.0.5 fsspec-2024.6.1 pandas-2.2.2 pyarrow-17.0.0 pytz-2024.1 tzdata-2024.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Python\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install delta-sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install delta-sharing\n",
    "import delta_sharing\n",
    "\n",
    "# Point to the profile file. It can be a file on the local file system or a file on a remote storage.\n",
    "profile_file = \"<profile-file-path>\"\n",
    "\n",
    "# Create a SharingClient.\n",
    "client = delta_sharing.SharingClient(profile_file)\n",
    "\n",
    "# List all shared tables.\n",
    "client.list_all_tables()\n",
    "\n",
    "# Create a url to access a shared table.\n",
    "# A table path is the profile file path following with `#` and the fully qualified name of a table \n",
    "# (`<share-name>.<schema-name>.<table-name>`).\n",
    "table_url = profile_file + \"#<share-name>.<schema-name>.<table-name>\"\n",
    "\n",
    "# Fetch 10 rows from a table and convert it to a Pandas DataFrame. This can be used to read sample data \n",
    "# from a table that cannot fit in the memory.\n",
    "delta_sharing.load_as_pandas(table_url, limit=10)\n",
    "\n",
    "# Load a table as a Pandas DataFrame. This can be used to process tables that can fit in the memory.\n",
    "delta_sharing.load_as_pandas(table_url)\n",
    "\n",
    "# If the code is running with PySpark, you can use `load_as_spark` to load the table as a Spark DataFrame.\n",
    "delta_sharing.load_as_spark(table_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\lib\\site-packages\\google\\auth\\_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset namu_wiki already exists in asia-northeast3\n",
      "Table art_info_url already exists in dataset namu_wiki\n",
      "Loaded 104 rows into wev-dev-analytics:namu_wiki.art_info_url\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from namu_loader import NamuLoader\n",
    "import textwrap\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "import openai\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_community import BigQueryVectorStore\n",
    "\n",
    "# BigQuery\n",
    "def load_data_to_bigquery(client, json_data, project_id, dataset_id, table_id, region, write_disposition):\n",
    "    \n",
    "    # print(json_data)\n",
    "    \n",
    "    # metadata 는 한글이 섞여있으므로 ensure_ascii 옵션을 False 로 설정한다.\n",
    "    # artist_info, page_url 은 크롤링된 정보에서 가져오는 것이 아니므로 수동으로 넣어준다.\n",
    "    # for item in json_data:\n",
    "    #     item['metadata'] = json.dumps(item['metadata'], ensure_ascii=False)\n",
    "    #     item['artist_info'] = artist_info\n",
    "    #     item['page_url'] = page_url\n",
    "    \n",
    "    table_ref = client.dataset(dataset_id, project=project_id).table(table_id)\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.write_disposition = write_disposition\n",
    "    \n",
    "    load_job = client.load_table_from_json(\n",
    "        json_data, table_ref, location=region, job_config=job_config\n",
    "    )\n",
    "    \n",
    "    load_job.result()  \n",
    "    print(f'Loaded {len(json_data)} rows into {project_id}:{dataset_id}.{table_id}')\n",
    "\n",
    "def main():\n",
    "\n",
    "\t  # # NamuLoader 를 사용해서 url 정보를 크롤링한다.\n",
    "    # url = 'https://namu.wiki/w/(%EC%97%AC%EC%9E%90)%EC%95%84%EC%9D%B4%EB%93%A4?from=%EC%97%AC%EC%9E%90%EC%95%84%EC%9D%B4%EB%93%A4'\n",
    "    # max_hop = 1\n",
    "    # verbose = True\n",
    "    # loader = NamuLoader(url=url, max_hop=max_hop, verbose=verbose)\n",
    "\n",
    "\t\t# # 크롤링한 데이터를 documents 에 append \n",
    "    # documents = []\n",
    "    # for doc in loader.lazy_load():\n",
    "    #     documents.append({\n",
    "    #         \"page_content\": doc.page_content,\n",
    "    #         \"metadata\": doc.metadata\n",
    "    #     })\n",
    "    \n",
    "    # 내가 작업하고자 하는 GCP 프로젝트, region, dataset, table id 설정\n",
    "    PROJECT_ID = \"wev-dev-analytics\"\n",
    "    REGION = \"asia-northeast3\"\n",
    "    DATASET_ID = \"namu_wiki\"\n",
    "    TABLE_ID = \"art_info_url\"\n",
    "\n",
    "    # 빅쿼리에 저장할 테이블의 schema 정의\n",
    "    client = bigquery.Client(project=PROJECT_ID)\n",
    "    schema = [\n",
    "      bigquery.SchemaField(\"we_art_id\", \"INT\"),\n",
    "      bigquery.SchemaField(\"we_art_name\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"url\", \"STRING\"),\n",
    "      ]\n",
    "    \n",
    "    dataset_ref = client.dataset(DATASET_ID)\n",
    "    dataset = bigquery.Dataset(dataset_ref)\n",
    "    dataset.location = REGION\n",
    "\n",
    "    # 데이터셋 생성 (이미 존재하는 경우 생략)\n",
    "    try:\n",
    "        client.create_dataset(dataset)\n",
    "        print(f\"Created dataset {DATASET_ID} in {REGION}\")\n",
    "    except:\n",
    "        print(f\"Dataset {DATASET_ID} already exists in {REGION}\")\n",
    "    \n",
    "    # 테이블 생성 (이미 존재하는 경우 생략)\n",
    "    table_ref = dataset_ref.table(TABLE_ID)\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "\n",
    "    try:\n",
    "        client.create_table(table)\n",
    "        print(f\"Created table {TABLE_ID} in dataset {DATASET_ID}\")\n",
    "    except:\n",
    "        print(f\"Table {TABLE_ID} already exists in dataset {DATASET_ID}\")\n",
    "\n",
    "\t\t# # 넣고 싶은 ARTIST_INFO, PAGE_URL 값을 기입해준다.\n",
    "    # ARTIST_INFO = \"(여자)아이들\"\n",
    "    # PAGE_URL = url\n",
    "\n",
    "\t\t# 각 파라미터를 기입해준다. WRITE_APPEND 은 테이블에 데이터가 append 되고, WRITE_TRUNCATE 을 기입하면 overwrite 된다.\n",
    "    load_data_to_bigquery(client, we_artist2.to_dict(orient='records'), PROJECT_ID, DATASET_ID, TABLE_ID, REGION, bigquery.WriteDisposition.WRITE_APPEND) # WRITE_APPEND, WRITE_TRUNCATE\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login&state=KRhYdICFSWNp8wSH5siCCfmFPJmA7Q&access_type=offline&code_challenge=Jk8BS9ZvgQf7Pkjk9G00qvTDuvKXUQQ2LDt8zVg-Sq4&code_challenge_method=S256\n",
      "\n",
      "\n",
      "Credentials saved to file: [C:\\Users\\sanghyoon\\AppData\\Roaming\\gcloud\\application_default_credentials.json]\n",
      "\n",
      "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
      "WARNING: \n",
      "Cannot add the project \"wev-dev-analytics\" to ADC as the quota project because the account in ADC does not have the \"serviceusage.services.use\" permission on this project. You might receive a \"quota_exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "we_artist2 = pd.read_csv('C:\\\\Users\\\\sanghyoon\\\\Desktop\\\\we_artist2.csv', encoding= 'UTF-8', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
