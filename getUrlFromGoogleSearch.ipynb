{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # .env 파일에서 환경 변수를 불러옵니다.\n",
    "\n",
    "# api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# print(f\"API Key: {api_key}\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "search_engine_key = os.getenv(\"GOOGLE_SEARCH_ENGINE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   we_art_id    we_art_name\n",
      "0        114    CHOI SOO HO\n",
      "1        115           CHUU\n",
      "2        116  HI-FI UN!CORN\n",
      "3        117   JANG KI YONG\n",
      "4        118      WEKI MEKI\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 파일 경로\n",
    "file_path = 'C:\\\\Users\\\\sanghyoon\\\\Desktop\\\\we_artist_1.csv'\n",
    "\n",
    "# CSV 파일 읽기 (ISO-8859-1 인코딩 사용)\n",
    "we_artist = pd.read_csv(file_path, encoding='UTF-8')\n",
    "\n",
    "# 읽은 데이터 출력\n",
    "print(we_artist.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-bigquery in c:\\python\\lib\\site-packages (from -r requirements.txt (line 1)) (3.25.0)\n",
      "Requirement already satisfied: openai in c:\\python\\lib\\site-packages (from -r requirements.txt (line 2)) (1.37.1)\n",
      "Requirement already satisfied: langchain-community in c:\\python\\lib\\site-packages (from -r requirements.txt (line 3)) (0.2.7)\n",
      "Requirement already satisfied: langchain-google-vertexai in c:\\python\\lib\\site-packages (from -r requirements.txt (line 4)) (1.0.7)\n",
      "Requirement already satisfied: langchain-google-community in c:\\python\\lib\\site-packages (from -r requirements.txt (line 5)) (1.0.7)\n",
      "Requirement already satisfied: requests in c:\\python\\lib\\site-packages (from -r requirements.txt (line 6)) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\python\\lib\\site-packages (from -r requirements.txt (line 7)) (4.12.3)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in c:\\python\\lib\\site-packages (from google-cloud-bigquery->-r requirements.txt (line 1)) (2.32.0)\n",
      "Requirement already satisfied: packaging>=20.0.0 in c:\\users\\sanghyoon\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-bigquery->-r requirements.txt (line 1)) (24.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in c:\\python\\lib\\site-packages (from google-cloud-bigquery->-r requirements.txt (line 1)) (2.4.1)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in c:\\python\\lib\\site-packages (from google-cloud-bigquery->-r requirements.txt (line 1)) (2.19.1)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in c:\\users\\sanghyoon\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-bigquery->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in c:\\python\\lib\\site-packages (from google-cloud-bigquery->-r requirements.txt (line 1)) (2.7.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\python\\lib\\site-packages (from openai->-r requirements.txt (line 2)) (0.27.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\sanghyoon\\appdata\\roaming\\python\\python310\\site-packages (from openai->-r requirements.txt (line 2)) (4.12.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\python\\lib\\site-packages (from openai->-r requirements.txt (line 2)) (1.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\python\\lib\\site-packages (from openai->-r requirements.txt (line 2)) (4.4.0)\n",
      "Requirement already satisfied: sniffio in c:\\python\\lib\\site-packages (from openai->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\python\\lib\\site-packages (from openai->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: tqdm>4 in c:\\python\\lib\\site-packages (from openai->-r requirements.txt (line 2)) (4.66.4)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.7 in c:\\python\\lib\\site-packages (from langchain-community->-r requirements.txt (line 3)) (0.2.7)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\python\\lib\\site-packages (from langchain-community->-r requirements.txt (line 3)) (0.1.84)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\python\\lib\\site-packages (from langchain-community->-r requirements.txt (line 3)) (6.0.1)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.12 in c:\\python\\lib\\site-packages (from langchain-community->-r requirements.txt (line 3)) (0.2.24)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\python\\lib\\site-packages (from langchain-community->-r requirements.txt (line 3)) (0.6.7)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\python\\lib\\site-packages (from langchain-community->-r requirements.txt (line 3)) (3.9.5)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\python\\lib\\site-packages (from langchain-community->-r requirements.txt (line 3)) (1.26.4)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\python\\lib\\site-packages (from langchain-community->-r requirements.txt (line 3)) (8.3.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\python\\lib\\site-packages (from langchain-community->-r requirements.txt (line 3)) (2.0.31)\n",
      "Requirement already satisfied: google-cloud-aiplatform<2.0.0,>=1.56.0 in c:\\python\\lib\\site-packages (from langchain-google-vertexai->-r requirements.txt (line 4)) (1.60.0)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0,>=2.17.0 in c:\\python\\lib\\site-packages (from langchain-google-vertexai->-r requirements.txt (line 4)) (2.18.0)\n",
      "Requirement already satisfied: google-api-python-client<3.0.0,>=2.122.0 in c:\\python\\lib\\site-packages (from langchain-google-community->-r requirements.txt (line 5)) (2.137.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.62.0 in c:\\python\\lib\\site-packages (from langchain-google-community->-r requirements.txt (line 5)) (1.65.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\lib\\site-packages (from requests->-r requirements.txt (line 6)) (2024.7.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python\\lib\\site-packages (from requests->-r requirements.txt (line 6)) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python\\lib\\site-packages (from requests->-r requirements.txt (line 6)) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python\\lib\\site-packages (from requests->-r requirements.txt (line 6)) (2.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\python\\lib\\site-packages (from beautifulsoup4->-r requirements.txt (line 7)) (2.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\python\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\python\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 3)) (6.0.5)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\python\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 3)) (4.0.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\python\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 3)) (1.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\python\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 3)) (23.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\python\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 3)) (1.9.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\sanghyoon\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 2)) (1.2.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\python\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 3)) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\python\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 3)) (3.21.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\python\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery->-r requirements.txt (line 1)) (1.24.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\python\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery->-r requirements.txt (line 1)) (1.63.2)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in c:\\python\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery->-r requirements.txt (line 1)) (4.25.4)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\python\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery->-r requirements.txt (line 1)) (1.62.2)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in c:\\python\\lib\\site-packages (from google-api-python-client<3.0.0,>=2.122.0->langchain-google-community->-r requirements.txt (line 5)) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\python\\lib\\site-packages (from google-api-python-client<3.0.0,>=2.122.0->langchain-google-community->-r requirements.txt (line 5)) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\python\\lib\\site-packages (from google-api-python-client<3.0.0,>=2.122.0->langchain-google-community->-r requirements.txt (line 5)) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\python\\lib\\site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery->-r requirements.txt (line 1)) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\python\\lib\\site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery->-r requirements.txt (line 1)) (5.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\python\\lib\\site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery->-r requirements.txt (line 1)) (0.4.0)\n",
      "Requirement already satisfied: shapely<3.0.0dev in c:\\python\\lib\\site-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai->-r requirements.txt (line 4)) (2.0.5)\n",
      "Requirement already satisfied: docstring-parser<1 in c:\\python\\lib\\site-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai->-r requirements.txt (line 4)) (0.16)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in c:\\python\\lib\\site-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai->-r requirements.txt (line 4)) (1.12.4)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in c:\\python\\lib\\site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai->-r requirements.txt (line 4)) (0.13.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\python\\lib\\site-packages (from google-cloud-storage<3.0.0,>=2.17.0->langchain-google-vertexai->-r requirements.txt (line 4)) (1.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\python\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client<3.0.0,>=2.122.0->langchain-google-community->-r requirements.txt (line 5)) (3.1.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\python\\lib\\site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 2)) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\python\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 2)) (0.14.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\python\\lib\\site-packages (from langchain<0.3.0,>=0.2.7->langchain-community->-r requirements.txt (line 3)) (0.2.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\python\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.12->langchain-community->-r requirements.txt (line 3)) (1.33)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\python\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.12->langchain-community->-r requirements.txt (line 3)) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\python\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community->-r requirements.txt (line 3)) (3.10.6)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\python\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery->-r requirements.txt (line 1)) (0.6.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\python\\lib\\site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\python\\lib\\site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 2)) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sanghyoon\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\python\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community->-r requirements.txt (line 3)) (3.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\sanghyoon\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>4->openai->-r requirements.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\python\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 3)) (1.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Python\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 114 CHOI SOO HO\n",
      "https://namu.wiki/w/최수호(가수)\n",
      "1 115 CHUU\n",
      "https://namu.wiki/w/츄(가수)\n",
      "2 116 HI-FI UN!CORN\n",
      "https://namu.wiki/w/Over%20the%20Rainbow(Hi-Fi%20Un!corn)\n",
      "3 117 JANG KI YONG\n",
      "https://namu.wiki/w/장기용\n",
      "4 118 WEKI MEKI\n",
      "https://namu.wiki/w/위키미키\n",
      "5 119 KATSEYE\n",
      "https://namu.wiki/w/KATSEYE\n",
      "6 120 CHANMINA\n",
      "https://namu.wiki/w/챤미나\n",
      "7 121 KIM MYUNGSOO(L)\n",
      "https://namu.wiki/w/엘(인피니트)\n",
      "8 122 KANGTA\n",
      "https://namu.wiki/w/강타\n",
      "9 123 BOA\n",
      "https://namu.wiki/w/보아\n",
      "10 124 TVXQ!\n",
      "https://namu.wiki/w/동방신기\n",
      "11 125 SUPER JUNIOR\n",
      "https://namu.wiki/w/SUPER%20JUNIOR\n",
      "12 126 GIRLS' GENERATION\n",
      "https://namu.wiki/w/소녀시대\n",
      "13 127 SHINEE\n",
      "https://namu.wiki/w/SHINee\n",
      "14 128 RED VELVET\n",
      "https://namu.wiki/w/Red%20Velvet\n",
      "15 129 EXO\n",
      "https://namu.wiki/w/EXO\n",
      "16 130 NCT 127\n",
      "https://namu.wiki/w/NCT%20127\n",
      "17 131 NCT DREAM\n",
      "https://namu.wiki/w/NCT%20DREAM\n",
      "18 132 WAYV\n",
      "https://namu.wiki/w/WayV\n",
      "19 133 AESPA\n",
      "https://namu.wiki/w/aespa\n",
      "20 134 RIIZE\n",
      "https://namu.wiki/w/RIIZE\n",
      "21 135 SMTOWN\n",
      "https://namu.wiki/w/강타%20데뷔%2025주년%20프로젝트\n",
      "22 136 SM_CONCERT\n",
      "https://namu.wiki/w/M.I.L.K.\n",
      "23 137 LEE SUNG KYOUNG\n",
      "https://namu.wiki/w/이성경\n",
      "24 138 AHN HYO SEOP\n",
      "https://namu.wiki/w/안효섭\n",
      "25 139 QWER\n",
      "https://namu.wiki/w/QWER\n",
      "26 140 NCT WISH\n",
      "https://namu.wiki/w/NCT%20WISH\n",
      "27 141 WHIB\n",
      "https://namu.wiki/w/유건(WHIB)\n",
      "28 142 AMPERS&ONE\n",
      "https://namu.wiki/w/앰퍼샌드원\n",
      "29 1593 KYUHYUN\n",
      "https://namu.wiki/w/규현\n",
      "30 1594 JD1\n",
      "https://namu.wiki/w/정동원\n",
      "31 1595 BULLET TRAIN\n",
      "https://namu.wiki/w/초특급\n",
      "32 1596 SUPER★DRAGON\n",
      "https://namu.wiki/w/SUPER★DRAGON\n",
      "33 1597 ONE N' ONLY\n",
      "https://namu.wiki/w/ONE%20N'%20ONLY\n",
      "34 1598 LIENEL\n",
      "https://namu.wiki/w/Lienel\n",
      "35 1629 PLAVE\n",
      "https://namu.wiki/w/PLAVE\n",
      "36 1630 TWS\n",
      "https://namu.wiki/w/TWS\n",
      "37 1631 BABYMONSTER\n",
      "https://namu.wiki/w/BABYMONSTER\n",
      "38 1635 NIGHTLY\n",
      "https://namu.wiki/w/키이라%20나이틀리\n",
      "39 1636 BANG YEDAM\n",
      "https://namu.wiki/w/방예담\n",
      "40 1637 LUCAS\n",
      "https://namu.wiki/w/루카스(가수)\n",
      "41 1638 CONAN GRAY\n",
      "https://namu.wiki/w/코난%20그레이\n",
      "42 1639 UNIS\n",
      "https://namu.wiki/w/UNIS\n",
      "43 1641 THUY\n",
      "https://namu.wiki/w/레팜투이띠엔\n",
      "44 1642 NOWADAYS\n",
      "https://namu.wiki/w/NOWADAYS\n",
      "45 1643 UMI\n",
      "https://namu.wiki/w/시노노메%20우미\n",
      "46 1644 YUNGBLUD\n",
      "https://namu.wiki/w/YUNGBLUD\n",
      "47 1645 SYUDOU\n",
      "https://namu.wiki/w/syudou\n",
      "48 1646 AYUMU IMAZU\n",
      "https://namu.wiki/w/쇼타로\n",
      "49 1648 NOA\n",
      "https://namu.wiki/w/NOA\n",
      "50 1649 GIRLS ON FIRE\n",
      "https://namu.wiki/w/걸스%20온%20파이어/참가자\n",
      "51 1650 KIM WOOJIN\n",
      "https://namu.wiki/w/김우진(가수)\n",
      "52 1651 EILL\n",
      "https://namu.wiki/w/eill\n",
      "53 1652 ONEW\n",
      "https://namu.wiki/w/VOICE(온유)\n",
      "54 1653 LAUV\n",
      "https://namu.wiki/w/라우브\n",
      "55 1654 VVUP\n",
      "https://namu.wiki/w/VVUP\n",
      "56 1655 10CM\n",
      "https://namu.wiki/w/10CM\n",
      "57 1656 BIBI\n",
      "https://namu.wiki/w/BIBI\n",
      "58 1657 KINO\n",
      "https://namu.wiki/w/토키노%20소라\n",
      "59 1658 82MAJOR\n",
      "https://namu.wiki/w/82MAJOR\n",
      "60 1659 HONG SEOK\n",
      "https://namu.wiki/w/강홍석\n",
      "61 1660 YOASOBI\n",
      "https://namu.wiki/w/YOASOBI\n",
      "62 1661 BYEON WOO SEOK\n",
      "https://namu.wiki/w/변우석\n",
      "63 1662 WOOSEOK\n",
      "https://namu.wiki/w/변우석\n",
      "64 1663 CHEN\n",
      "https://namu.wiki/w/첸(EXO)\n",
      "65 1664 BAEKHYUN\n",
      "https://namu.wiki/w/백현\n",
      "66 1665 XIUMIN\n",
      "https://namu.wiki/w/Brand%20New(시우민)\n",
      "67 1666 WOLF'LO\n",
      "https://namu.wiki/w/스트릿%20우먼%20파이터%202\n",
      "68 1667 TIOT\n",
      "https://namu.wiki/w/TIOT\n",
      "69 1669 YOUNGTAK\n",
      "https://namu.wiki/w/영탁\n",
      "70 1670 ARIANA GRANDE\n",
      "https://namu.wiki/w/아리아나%20그란데\n",
      "71 1671 JVKE\n",
      "https://namu.wiki/w/JVKE\n",
      "72 1673 JEONG SUN AH\n",
      "https://namu.wiki/w/정선아(뮤지컬%20배우)\n"
     ]
    }
   ],
   "source": [
    "from get_namu_url import GetNamuUrl\n",
    "\n",
    "inst1 = GetNamuUrl(google_api_key, search_engine_key)\n",
    "\n",
    "# we_artist['url_link'] = we_artist.apply(lambda x:inst1.get_url(x['we_art_name']), axis = 1)\n",
    "\n",
    "we_artist2 = pd.DataFrame(columns=['we_art_id','we_art_name','url'])\n",
    "\n",
    "for idx, row in we_artist.iterrows():\n",
    "    print(idx, row['we_art_id'], row['we_art_name'])\n",
    "    url = inst1.get_url(row['we_art_name'])\n",
    "    print(url)\n",
    "    we_artist2 = pd.concat([we_artist2, pd.DataFrame([[row['we_art_id'], row['we_art_name'], url]], columns=['we_art_id','we_art_name','url'])])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting delta-sharing\n",
      "  Downloading delta_sharing-1.0.5-py3-none-any.whl (17 kB)\n",
      "Collecting pyarrow>=4.0.0\n",
      "  Downloading pyarrow-17.0.0-cp310-cp310-win_amd64.whl (25.1 MB)\n",
      "Collecting fsspec>=0.7.4\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\python\\lib\\site-packages (from delta-sharing) (3.9.5)\n",
      "Requirement already satisfied: requests in c:\\python\\lib\\site-packages (from delta-sharing) (2.32.3)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.2-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "Requirement already satisfied: yarl>=1.6.0 in c:\\python\\lib\\site-packages (from delta-sharing) (1.9.4)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\python\\lib\\site-packages (from pyarrow>=4.0.0->delta-sharing) (1.26.4)\n",
      "Requirement already satisfied: multidict>=4.0 in c:\\python\\lib\\site-packages (from yarl>=1.6.0->delta-sharing) (6.0.5)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\python\\lib\\site-packages (from yarl>=1.6.0->delta-sharing) (3.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\python\\lib\\site-packages (from aiohttp->delta-sharing) (23.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\python\\lib\\site-packages (from aiohttp->delta-sharing) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\python\\lib\\site-packages (from aiohttp->delta-sharing) (4.0.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\python\\lib\\site-packages (from aiohttp->delta-sharing) (1.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sanghyoon\\appdata\\roaming\\python\\python310\\site-packages (from pandas->delta-sharing) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sanghyoon\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas->delta-sharing) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python\\lib\\site-packages (from requests->delta-sharing) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python\\lib\\site-packages (from requests->delta-sharing) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\lib\\site-packages (from requests->delta-sharing) (2024.7.4)\n",
      "Installing collected packages: tzdata, pytz, pyarrow, pandas, fsspec, delta-sharing\n",
      "Successfully installed delta-sharing-1.0.5 fsspec-2024.6.1 pandas-2.2.2 pyarrow-17.0.0 pytz-2024.1 tzdata-2024.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Python\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install delta-sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install delta-sharing\n",
    "import delta_sharing\n",
    "\n",
    "# Point to the profile file. It can be a file on the local file system or a file on a remote storage.\n",
    "profile_file = \"<profile-file-path>\"\n",
    "\n",
    "# Create a SharingClient.\n",
    "client = delta_sharing.SharingClient(profile_file)\n",
    "\n",
    "# List all shared tables.\n",
    "client.list_all_tables()\n",
    "\n",
    "# Create a url to access a shared table.\n",
    "# A table path is the profile file path following with `#` and the fully qualified name of a table \n",
    "# (`<share-name>.<schema-name>.<table-name>`).\n",
    "table_url = profile_file + \"#<share-name>.<schema-name>.<table-name>\"\n",
    "\n",
    "# Fetch 10 rows from a table and convert it to a Pandas DataFrame. This can be used to read sample data \n",
    "# from a table that cannot fit in the memory.\n",
    "delta_sharing.load_as_pandas(table_url, limit=10)\n",
    "\n",
    "# Load a table as a Pandas DataFrame. This can be used to process tables that can fit in the memory.\n",
    "delta_sharing.load_as_pandas(table_url)\n",
    "\n",
    "# If the code is running with PySpark, you can use `load_as_spark` to load the table as a Spark DataFrame.\n",
    "delta_sharing.load_as_spark(table_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "we_artist2.to_csv('C:\\\\Users\\\\sanghyoon\\\\Desktop\\\\we_artist2_1.csv', encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\lib\\site-packages\\google\\auth\\_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset namu_wiki already exists in asia-northeast3\n",
      "Table art_info_url already exists in dataset namu_wiki\n",
      "Loaded 177 rows into wev-dev-analytics:namu_wiki.art_info_url\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from namu_loader import NamuLoader\n",
    "import textwrap\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "import openai\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_community import BigQueryVectorStore\n",
    "\n",
    "# BigQuery\n",
    "def load_data_to_bigquery(client, json_data, project_id, dataset_id, table_id, region, write_disposition):\n",
    "    \n",
    "    # print(json_data)\n",
    "    \n",
    "    # metadata 는 한글이 섞여있으므로 ensure_ascii 옵션을 False 로 설정한다.\n",
    "    # artist_info, page_url 은 크롤링된 정보에서 가져오는 것이 아니므로 수동으로 넣어준다.\n",
    "    # for item in json_data:\n",
    "    #     item['metadata'] = json.dumps(item['metadata'], ensure_ascii=False)\n",
    "    #     item['artist_info'] = artist_info\n",
    "    #     item['page_url'] = page_url\n",
    "    \n",
    "    table_ref = client.dataset(dataset_id, project=project_id).table(table_id)\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.write_disposition = write_disposition\n",
    "    \n",
    "    load_job = client.load_table_from_json(\n",
    "        json_data, table_ref, location=region, job_config=job_config\n",
    "    )\n",
    "    \n",
    "    load_job.result()  \n",
    "    print(f'Loaded {len(json_data)} rows into {project_id}:{dataset_id}.{table_id}')\n",
    "\n",
    "def main():\n",
    "\n",
    "\t  # # NamuLoader 를 사용해서 url 정보를 크롤링한다.\n",
    "    # url = 'https://namu.wiki/w/(%EC%97%AC%EC%9E%90)%EC%95%84%EC%9D%B4%EB%93%A4?from=%EC%97%AC%EC%9E%90%EC%95%84%EC%9D%B4%EB%93%A4'\n",
    "    # max_hop = 1\n",
    "    # verbose = True\n",
    "    # loader = NamuLoader(url=url, max_hop=max_hop, verbose=verbose)\n",
    "\n",
    "\t\t# # 크롤링한 데이터를 documents 에 append \n",
    "    # documents = []\n",
    "    # for doc in loader.lazy_load():\n",
    "    #     documents.append({\n",
    "    #         \"page_content\": doc.page_content,\n",
    "    #         \"metadata\": doc.metadata\n",
    "    #     })\n",
    "    \n",
    "    # 내가 작업하고자 하는 GCP 프로젝트, region, dataset, table id 설정\n",
    "    PROJECT_ID = \"wev-dev-analytics\"\n",
    "    REGION = \"asia-northeast3\"\n",
    "    DATASET_ID = \"namu_wiki\"\n",
    "    TABLE_ID = \"art_info_url\"\n",
    "\n",
    "    # 빅쿼리에 저장할 테이블의 schema 정의\n",
    "    client = bigquery.Client(project=PROJECT_ID)\n",
    "    schema = [\n",
    "      bigquery.SchemaField(\"we_art_id\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"we_art_name\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"url\", \"STRING\"),\n",
    "      ]\n",
    "    \n",
    "    dataset_ref = client.dataset(DATASET_ID)\n",
    "    dataset = bigquery.Dataset(dataset_ref)\n",
    "    dataset.location = REGION\n",
    "\n",
    "    # 데이터셋 생성 (이미 존재하는 경우 생략)\n",
    "    try:\n",
    "        client.create_dataset(dataset)\n",
    "        print(f\"Created dataset {DATASET_ID} in {REGION}\")\n",
    "    except:\n",
    "        print(f\"Dataset {DATASET_ID} already exists in {REGION}\")\n",
    "    \n",
    "    # 테이블 생성 (이미 존재하는 경우 생략)\n",
    "    table_ref = dataset_ref.table(TABLE_ID)\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "\n",
    "    try:\n",
    "        client.create_table(table)\n",
    "        print(f\"Created table {TABLE_ID} in dataset {DATASET_ID}\")\n",
    "    except:\n",
    "        print(f\"Table {TABLE_ID} already exists in dataset {DATASET_ID}\")\n",
    "\n",
    "\t\t# # 넣고 싶은 ARTIST_INFO, PAGE_URL 값을 기입해준다.\n",
    "    # ARTIST_INFO = \"(여자)아이들\"\n",
    "    # PAGE_URL = url\n",
    "\n",
    "\t\t# 각 파라미터를 기입해준다. WRITE_APPEND 은 테이블에 데이터가 append 되고, WRITE_TRUNCATE 을 기입하면 overwrite 된다.\n",
    "    load_data_to_bigquery(client, we_artist2.to_dict(orient='records'), PROJECT_ID, DATASET_ID, TABLE_ID, REGION, bigquery.WriteDisposition.WRITE_APPEND) # WRITE_APPEND, WRITE_TRUNCATE\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login&state=KRhYdICFSWNp8wSH5siCCfmFPJmA7Q&access_type=offline&code_challenge=Jk8BS9ZvgQf7Pkjk9G00qvTDuvKXUQQ2LDt8zVg-Sq4&code_challenge_method=S256\n",
      "\n",
      "\n",
      "Credentials saved to file: [C:\\Users\\sanghyoon\\AppData\\Roaming\\gcloud\\application_default_credentials.json]\n",
      "\n",
      "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
      "WARNING: \n",
      "Cannot add the project \"wev-dev-analytics\" to ADC as the quota project because the account in ADC does not have the \"serviceusage.services.use\" permission on this project. You might receive a \"quota_exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "we_artist2 = pd.read_csv('C:\\\\Users\\\\sanghyoon\\\\Desktop\\\\we_artist3.csv', encoding= 'UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
