{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # .env 파일에서 환경 변수를 불러옵니다.\n",
    "\n",
    "# api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# print(f\"API Key: {api_key}\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "search_engine_key = os.getenv(\"GOOGLE_SEARCH_ENGINE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'905d76e8becfd4a83'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"GOOGLE_SEARCH_ENGINE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   we_art_id we_art_name\n",
      "0          1         TXT\n",
      "1          2         BTS\n",
      "2          3     GFRIEND\n",
      "3          4   SEVENTEEN\n",
      "4          5     ENHYPEN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 파일 경로\n",
    "file_path = 'C:\\\\Users\\\\sanghyoon\\\\Desktop\\\\we_artist.csv'\n",
    "\n",
    "# CSV 파일 읽기 (ISO-8859-1 인코딩 사용)\n",
    "we_artist = pd.read_csv(file_path, encoding='UTF-8')\n",
    "\n",
    "# 읽은 데이터 출력\n",
    "print(we_artist.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-bigquery in c:\\python\\lib\\site-packages (from -r requirements.txt (line 1)) (3.25.0)\n",
      "Requirement already satisfied: openai in c:\\python\\lib\\site-packages (from -r requirements.txt (line 2)) (1.37.1)\n",
      "Requirement already satisfied: langchain-community in c:\\python\\lib\\site-packages (from -r requirements.txt (line 3)) (0.2.7)\n",
      "Requirement already satisfied: langchain-google-vertexai in c:\\python\\lib\\site-packages (from -r requirements.txt (line 4)) (1.0.7)\n",
      "Requirement already satisfied: langchain-google-community in c:\\python\\lib\\site-packages (from -r requirements.txt (line 5)) (1.0.7)\n",
      "Requirement already satisfied: requests in c:\\python\\lib\\site-packages (from -r requirements.txt (line 6)) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\python\\lib\\site-packages (from -r requirements.txt (line 7)) (4.12.3)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in c:\\users\\sanghyoon\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-bigquery->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: packaging>=20.0.0 in c:\\users\\sanghyoon\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-bigquery->-r requirements.txt (line 1)) (24.1)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in c:\\python\\lib\\site-packages (from google-cloud-bigquery->-r requirements.txt (line 1)) (2.19.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in c:\\python\\lib\\site-packages (from google-cloud-bigquery->-r requirements.txt (line 1)) (2.7.1)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in c:\\python\\lib\\site-packages (from google-cloud-bigquery->-r requirements.txt (line 1)) (2.32.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in c:\\python\\lib\\site-packages (from google-cloud-bigquery->-r requirements.txt (line 1)) (2.4.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\sanghyoon\\appdata\\roaming\\python\\python310\\site-packages (from openai->-r requirements.txt (line 2)) (4.12.2)\n",
      "Requirement already satisfied: tqdm>4 in c:\\python\\lib\\site-packages (from openai->-r requirements.txt (line 2)) (4.66.4)\n",
      "Requirement already satisfied: sniffio in c:\\python\\lib\\site-packages (from openai->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\python\\lib\\site-packages (from openai->-r requirements.txt (line 2)) (1.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\python\\lib\\site-packages (from openai->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\python\\lib\\site-packages (from openai->-r requirements.txt (line 2)) (4.4.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\python\\lib\\site-packages (from openai->-r requirements.txt (line 2)) (0.27.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\python\\lib\\site-packages (from langchain-community->-r requirements.txt (line 3)) (6.0.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\python\\lib\\site-packages (from langchain-community->-r requirements.txt (line 3)) (0.6.7)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.7 in c:\\python\\lib\\site-packages (from langchain-community->-r requirements.txt (line 3)) (0.2.7)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.12 in c:\\python\\lib\\site-packages (from langchain-community->-r requirements.txt (line 3)) (0.2.24)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\python\\lib\\site-packages (from langchain-community->-r requirements.txt (line 3)) (2.0.31)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\python\\lib\\site-packages (from langchain-community->-r requirements.txt (line 3)) (1.26.4)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\python\\lib\\site-packages (from langchain-community->-r requirements.txt (line 3)) (0.1.84)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\python\\lib\\site-packages (from langchain-community->-r requirements.txt (line 3)) (3.9.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\python\\lib\\site-packages (from langchain-community->-r requirements.txt (line 3)) (8.3.0)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0,>=2.17.0 in c:\\python\\lib\\site-packages (from langchain-google-vertexai->-r requirements.txt (line 4)) (2.18.0)\n",
      "Requirement already satisfied: google-cloud-aiplatform<2.0.0,>=1.56.0 in c:\\python\\lib\\site-packages (from langchain-google-vertexai->-r requirements.txt (line 4)) (1.60.0)\n",
      "Requirement already satisfied: google-api-python-client<3.0.0,>=2.122.0 in c:\\python\\lib\\site-packages (from langchain-google-community->-r requirements.txt (line 5)) (2.137.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.62.0 in c:\\python\\lib\\site-packages (from langchain-google-community->-r requirements.txt (line 5)) (1.65.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\lib\\site-packages (from requests->-r requirements.txt (line 6)) (2024.7.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python\\lib\\site-packages (from requests->-r requirements.txt (line 6)) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python\\lib\\site-packages (from requests->-r requirements.txt (line 6)) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python\\lib\\site-packages (from requests->-r requirements.txt (line 6)) (2.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\python\\lib\\site-packages (from beautifulsoup4->-r requirements.txt (line 7)) (2.5)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\python\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 3)) (6.0.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\python\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\python\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 3)) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\python\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 3)) (1.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\python\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 3)) (23.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\python\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 3)) (4.0.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\sanghyoon\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 2)) (1.2.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\python\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 3)) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\python\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 3)) (3.21.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\python\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery->-r requirements.txt (line 1)) (1.63.2)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in c:\\python\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery->-r requirements.txt (line 1)) (4.25.4)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\python\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery->-r requirements.txt (line 1)) (1.24.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\python\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery->-r requirements.txt (line 1)) (1.62.2)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\python\\lib\\site-packages (from google-api-python-client<3.0.0,>=2.122.0->langchain-google-community->-r requirements.txt (line 5)) (4.1.1)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\python\\lib\\site-packages (from google-api-python-client<3.0.0,>=2.122.0->langchain-google-community->-r requirements.txt (line 5)) (0.2.0)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in c:\\python\\lib\\site-packages (from google-api-python-client<3.0.0,>=2.122.0->langchain-google-community->-r requirements.txt (line 5)) (0.22.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\python\\lib\\site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery->-r requirements.txt (line 1)) (5.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\python\\lib\\site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery->-r requirements.txt (line 1)) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\python\\lib\\site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery->-r requirements.txt (line 1)) (0.4.0)\n",
      "Requirement already satisfied: docstring-parser<1 in c:\\python\\lib\\site-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai->-r requirements.txt (line 4)) (0.16)\n",
      "Requirement already satisfied: shapely<3.0.0dev in c:\\python\\lib\\site-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai->-r requirements.txt (line 4)) (2.0.5)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in c:\\python\\lib\\site-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai->-r requirements.txt (line 4)) (1.12.4)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in c:\\python\\lib\\site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai->-r requirements.txt (line 4)) (0.13.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\python\\lib\\site-packages (from google-cloud-storage<3.0.0,>=2.17.0->langchain-google-vertexai->-r requirements.txt (line 4)) (1.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\python\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client<3.0.0,>=2.122.0->langchain-google-community->-r requirements.txt (line 5)) (3.1.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\python\\lib\\site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 2)) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\python\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 2)) (0.14.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\python\\lib\\site-packages (from langchain<0.3.0,>=0.2.7->langchain-community->-r requirements.txt (line 3)) (0.2.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\python\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.12->langchain-community->-r requirements.txt (line 3)) (1.33)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\python\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.12->langchain-community->-r requirements.txt (line 3)) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\python\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community->-r requirements.txt (line 3)) (3.10.6)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\python\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery->-r requirements.txt (line 1)) (0.6.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\python\\lib\\site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\python\\lib\\site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 2)) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sanghyoon\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\python\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community->-r requirements.txt (line 3)) (3.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\sanghyoon\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>4->openai->-r requirements.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\python\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 3)) (1.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Python\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 TXT\n"
     ]
    },
    {
     "ename": "HttpError",
     "evalue": "<HttpError 429 when requesting https://customsearch.googleapis.com/customsearch/v1?q=TXT&cx=905d76e8becfd4a83&siteSearch=https%3A%2F%2Fnamu.wiki%2Fw%2F&siteSearchFilter=i&cr=countryKR&hl=ko&orTerms=%EB%8D%B0%EB%B7%94&key=AIzaSyAtLvPjQ-WSF2A2n6cIhtVr86QM5CSiOS8&alt=json returned \"Quota exceeded for quota metric 'Queries' and limit 'Queries per day' of service 'customsearch.googleapis.com' for consumer 'project_number:700271639015'.\". Details: \"[{'message': \"Quota exceeded for quota metric 'Queries' and limit 'Queries per day' of service 'customsearch.googleapis.com' for consumer 'project_number:700271639015'.\", 'domain': 'global', 'reason': 'rateLimitExceeded'}]\">",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m we_artist\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(idx, row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwe_art_id\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwe_art_name\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 11\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[43minst1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwe_art_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(url)\n\u001b[0;32m     13\u001b[0m     we_artist2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([we_artist2, pd\u001b[38;5;241m.\u001b[39mDataFrame([[row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwe_art_id\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwe_art_name\u001b[39m\u001b[38;5;124m'\u001b[39m], url]], columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwe_art_id\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwe_art_name\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m])])\n",
      "File \u001b[1;32mc:\\Users\\sanghyoon\\Documents\\Github\\idol_info_finder\\idol_info_finder-2\\get_namu_url.py:33\u001b[0m, in \u001b[0;36mGetNamuUrl.get_url\u001b[1;34m(self, query, siteDomain)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m페이지 URL 반환\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     17\u001b[0m service \u001b[38;5;241m=\u001b[39m build(\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustomsearch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1\u001b[39m\u001b[38;5;124m\"\u001b[39m, developerKey\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoogle_api_key \n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     21\u001b[0m res \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     22\u001b[0m \u001b[43mservice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_engine_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43msiteSearch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msiteDomain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43msiteSearchFilter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcountryKR\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mko\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43morTerms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m데뷔\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m---> 33\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m )\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m     pprint\u001b[38;5;241m.\u001b[39mpprint(res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearchInformation\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\googleapiclient\\_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[0;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\googleapiclient\\http.py:938\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[1;34m(self, http, num_retries)\u001b[0m\n\u001b[0;32m    936\u001b[0m     callback(resp)\n\u001b[0;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpError(resp, content, uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muri)\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostproc(resp, content)\n",
      "\u001b[1;31mHttpError\u001b[0m: <HttpError 429 when requesting https://customsearch.googleapis.com/customsearch/v1?q=TXT&cx=905d76e8becfd4a83&siteSearch=https%3A%2F%2Fnamu.wiki%2Fw%2F&siteSearchFilter=i&cr=countryKR&hl=ko&orTerms=%EB%8D%B0%EB%B7%94&key=AIzaSyAtLvPjQ-WSF2A2n6cIhtVr86QM5CSiOS8&alt=json returned \"Quota exceeded for quota metric 'Queries' and limit 'Queries per day' of service 'customsearch.googleapis.com' for consumer 'project_number:700271639015'.\". Details: \"[{'message': \"Quota exceeded for quota metric 'Queries' and limit 'Queries per day' of service 'customsearch.googleapis.com' for consumer 'project_number:700271639015'.\", 'domain': 'global', 'reason': 'rateLimitExceeded'}]\">"
     ]
    }
   ],
   "source": [
    "from get_namu_url import GetNamuUrl\n",
    "\n",
    "inst1 = GetNamuUrl(google_api_key, search_engine_key)\n",
    "\n",
    "# we_artist['url_link'] = we_artist.apply(lambda x:inst1.get_url(x['we_art_name']), axis = 1)\n",
    "\n",
    "we_artist2 = pd.DataFrame(columns=['we_art_id','we_art_name','url'])\n",
    "\n",
    "for idx, row in we_artist.iterrows():\n",
    "    print(idx, row['we_art_id'], row['we_art_name'])\n",
    "    url = inst1.get_url(row['we_art_name'])\n",
    "    print(url)\n",
    "    we_artist2 = pd.concat([we_artist2, pd.DataFrame([[row['we_art_id'], row['we_art_name'], url]], columns=['we_art_id','we_art_name','url'])])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting delta-sharing\n",
      "  Downloading delta_sharing-1.0.5-py3-none-any.whl (17 kB)\n",
      "Collecting pyarrow>=4.0.0\n",
      "  Downloading pyarrow-17.0.0-cp310-cp310-win_amd64.whl (25.1 MB)\n",
      "Collecting fsspec>=0.7.4\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\python\\lib\\site-packages (from delta-sharing) (3.9.5)\n",
      "Requirement already satisfied: requests in c:\\python\\lib\\site-packages (from delta-sharing) (2.32.3)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.2-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "Requirement already satisfied: yarl>=1.6.0 in c:\\python\\lib\\site-packages (from delta-sharing) (1.9.4)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\python\\lib\\site-packages (from pyarrow>=4.0.0->delta-sharing) (1.26.4)\n",
      "Requirement already satisfied: multidict>=4.0 in c:\\python\\lib\\site-packages (from yarl>=1.6.0->delta-sharing) (6.0.5)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\python\\lib\\site-packages (from yarl>=1.6.0->delta-sharing) (3.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\python\\lib\\site-packages (from aiohttp->delta-sharing) (23.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\python\\lib\\site-packages (from aiohttp->delta-sharing) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\python\\lib\\site-packages (from aiohttp->delta-sharing) (4.0.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\python\\lib\\site-packages (from aiohttp->delta-sharing) (1.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sanghyoon\\appdata\\roaming\\python\\python310\\site-packages (from pandas->delta-sharing) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sanghyoon\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas->delta-sharing) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python\\lib\\site-packages (from requests->delta-sharing) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python\\lib\\site-packages (from requests->delta-sharing) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\lib\\site-packages (from requests->delta-sharing) (2024.7.4)\n",
      "Installing collected packages: tzdata, pytz, pyarrow, pandas, fsspec, delta-sharing\n",
      "Successfully installed delta-sharing-1.0.5 fsspec-2024.6.1 pandas-2.2.2 pyarrow-17.0.0 pytz-2024.1 tzdata-2024.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Python\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install delta-sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install delta-sharing\n",
    "import delta_sharing\n",
    "\n",
    "# Point to the profile file. It can be a file on the local file system or a file on a remote storage.\n",
    "profile_file = \"<profile-file-path>\"\n",
    "\n",
    "# Create a SharingClient.\n",
    "client = delta_sharing.SharingClient(profile_file)\n",
    "\n",
    "# List all shared tables.\n",
    "client.list_all_tables()\n",
    "\n",
    "# Create a url to access a shared table.\n",
    "# A table path is the profile file path following with `#` and the fully qualified name of a table \n",
    "# (`<share-name>.<schema-name>.<table-name>`).\n",
    "table_url = profile_file + \"#<share-name>.<schema-name>.<table-name>\"\n",
    "\n",
    "# Fetch 10 rows from a table and convert it to a Pandas DataFrame. This can be used to read sample data \n",
    "# from a table that cannot fit in the memory.\n",
    "delta_sharing.load_as_pandas(table_url, limit=10)\n",
    "\n",
    "# Load a table as a Pandas DataFrame. This can be used to process tables that can fit in the memory.\n",
    "delta_sharing.load_as_pandas(table_url)\n",
    "\n",
    "# If the code is running with PySpark, you can use `load_as_spark` to load the table as a Spark DataFrame.\n",
    "delta_sharing.load_as_spark(table_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from namu_loader import NamuLoader\n",
    "import textwrap\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "import openai\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_community import BigQueryVectorStore\n",
    "\n",
    "# BigQuery\n",
    "def load_data_to_bigquery(client, json_data, project_id, dataset_id, table_id, region, write_disposition):\n",
    "    \n",
    "    # metadata 는 한글이 섞여있으므로 ensure_ascii 옵션을 False 로 설정한다.\n",
    "    # artist_info, page_url 은 크롤링된 정보에서 가져오는 것이 아니므로 수동으로 넣어준다.\n",
    "    # for item in json_data:\n",
    "    #     item['metadata'] = json.dumps(item['metadata'], ensure_ascii=False)\n",
    "    #     item['artist_info'] = artist_info\n",
    "    #     item['page_url'] = page_url\n",
    "    \n",
    "    table_ref = client.dataset(dataset_id, project=project_id).table(table_id)\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.write_disposition = write_disposition\n",
    "    \n",
    "    load_job = client.load_table_from_json(\n",
    "        json_data, table_ref, location=region, job_config=job_config\n",
    "    )\n",
    "    \n",
    "    load_job.result()  \n",
    "    print(f'Loaded {len(json_data)} rows into {project_id}:{dataset_id}.{table_id}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'we_art_id': 1, 'we_art_name': 'TXT', 'url': 'url'},\n",
       " {'we_art_id': 2, 'we_art_name': 'BTS', 'url': 'url'},\n",
       " {'we_art_id': 3, 'we_art_name': 'GFRIEND', 'url': 'url'},\n",
       " {'we_art_id': 4, 'we_art_name': 'SEVENTEEN', 'url': 'url'},\n",
       " {'we_art_id': 5, 'we_art_name': 'ENHYPEN', 'url': 'url'},\n",
       " {'we_art_id': 6, 'we_art_name': \"NU'EST\", 'url': 'url'},\n",
       " {'we_art_id': 7, 'we_art_name': 'CL', 'url': 'url'},\n",
       " {'we_art_id': 8, 'we_art_name': 'P1HARMONY', 'url': 'url'},\n",
       " {'we_art_id': 9, 'we_art_name': 'WEEEKLY', 'url': 'url'},\n",
       " {'we_art_id': 10, 'we_art_name': 'SUNMI', 'url': 'url'},\n",
       " {'we_art_id': 12, 'we_art_name': 'HENRY', 'url': 'url'},\n",
       " {'we_art_id': 13, 'we_art_name': 'DREAMCATCHER', 'url': 'url'},\n",
       " {'we_art_id': 14, 'we_art_name': '2022 WEVERSE CON', 'url': 'url'},\n",
       " {'we_art_id': 15, 'we_art_name': 'GRACIE ABRAMS', 'url': 'url'},\n",
       " {'we_art_id': 16, 'we_art_name': 'CHERRY BULLET', 'url': 'url'},\n",
       " {'we_art_id': 17, 'we_art_name': 'NEW HOPE CLUB', 'url': 'url'},\n",
       " {'we_art_id': 18, 'we_art_name': 'ALEXANDER 23', 'url': 'url'},\n",
       " {'we_art_id': 19, 'we_art_name': 'MIRAE', 'url': 'url'},\n",
       " {'we_art_id': 20, 'we_art_name': 'TREASURE', 'url': 'url'},\n",
       " {'we_art_id': 21, 'we_art_name': 'HYBE INSIGHT', 'url': 'url'},\n",
       " {'we_art_id': 22, 'we_art_name': 'LETTEAMOR', 'url': 'url'},\n",
       " {'we_art_id': 23, 'we_art_name': 'JEREMY ZUCKER', 'url': 'url'},\n",
       " {'we_art_id': 24, 'we_art_name': 'PRETTYMUCH', 'url': 'url'},\n",
       " {'we_art_id': 25, 'we_art_name': 'WOOAH', 'url': 'url'},\n",
       " {'we_art_id': 26, 'we_art_name': 'MAX', 'url': 'url'},\n",
       " {'we_art_id': 27, 'we_art_name': 'FTISLAND', 'url': 'url'},\n",
       " {'we_art_id': 28, 'we_art_name': 'EVERGLOW', 'url': 'url'},\n",
       " {'we_art_id': 29, 'we_art_name': 'IKON', 'url': 'url'},\n",
       " {'we_art_id': 30, 'we_art_name': 'JUST B', 'url': 'url'},\n",
       " {'we_art_id': 31, 'we_art_name': 'MAD MONSTER', 'url': 'url'},\n",
       " {'we_art_id': 32, 'we_art_name': 'BLACKPINK', 'url': 'url'},\n",
       " {'we_art_id': 33, 'we_art_name': 'STAYC', 'url': 'url'},\n",
       " {'we_art_id': 34, 'we_art_name': 'LILHUDDY', 'url': 'url'},\n",
       " {'we_art_id': 35, 'we_art_name': 'PURPLE KISS', 'url': 'url'},\n",
       " {'we_art_id': 36, 'we_art_name': 'FROMIS_9', 'url': 'url'},\n",
       " {'we_art_id': 37, 'we_art_name': 'PRIKIL', 'url': 'url'},\n",
       " {'we_art_id': 38, 'we_art_name': 'WINNER', 'url': 'url'},\n",
       " {'we_art_id': 39, 'we_art_name': 'ONEUS', 'url': 'url'},\n",
       " {'we_art_id': 40, 'we_art_name': 'RAVI', 'url': 'url'},\n",
       " {'we_art_id': 41, 'we_art_name': 'KIMJUNSU', 'url': 'url'},\n",
       " {'we_art_id': 42, 'we_art_name': 'VERIVERY', 'url': 'url'},\n",
       " {'we_art_id': 43, 'we_art_name': 'UP10TION', 'url': 'url'},\n",
       " {'we_art_id': 44, 'we_art_name': '이진혁 (종료)', 'url': 'url'},\n",
       " {'we_art_id': 45, 'we_art_name': 'XG', 'url': 'url'},\n",
       " {'we_art_id': 46, 'we_art_name': 'LE SSERAFIM', 'url': 'url'},\n",
       " {'we_art_id': 47, 'we_art_name': 'BLITZERS', 'url': 'url'},\n",
       " {'we_art_id': 48, 'we_art_name': 'THE KINGDOM', 'url': 'url'},\n",
       " {'we_art_id': 49, 'we_art_name': 'YOON JISUNG', 'url': 'url'},\n",
       " {'we_art_id': 50, 'we_art_name': 'HWANG MIN HYUN', 'url': 'url'},\n",
       " {'we_art_id': 51, 'we_art_name': 'BAEKHO(KANG DONG HO)', 'url': 'url'},\n",
       " {'we_art_id': 52, 'we_art_name': 'APINK', 'url': 'url'},\n",
       " {'we_art_id': 53, 'we_art_name': 'VICTON', 'url': 'url'},\n",
       " {'we_art_id': 54, 'we_art_name': 'THE NEW SIX', 'url': 'url'},\n",
       " {'we_art_id': 55, 'we_art_name': 'SECRET NUMBER', 'url': 'url'},\n",
       " {'we_art_id': 56, 'we_art_name': '&TEAM', 'url': 'url'},\n",
       " {'we_art_id': 57, 'we_art_name': 'TRI.BE', 'url': 'url'},\n",
       " {'we_art_id': 58, 'we_art_name': 'ONLYONEOF', 'url': 'url'},\n",
       " {'we_art_id': 59, 'we_art_name': 'ROCKET PUNCH', 'url': 'url'},\n",
       " {'we_art_id': 60, 'we_art_name': 'HYOLYN', 'url': 'url'},\n",
       " {'we_art_id': 61, 'we_art_name': 'ATBO', 'url': 'url'},\n",
       " {'we_art_id': 62, 'we_art_name': 'GOLDEN CHILD', 'url': 'url'},\n",
       " {'we_art_id': 64, 'we_art_name': 'ZICO', 'url': 'url'},\n",
       " {'we_art_id': 69, 'we_art_name': 'NEWJEANS_PHONING', 'url': 'url'},\n",
       " {'we_art_id': 71, 'we_art_name': 'TFN', 'url': 'url'},\n",
       " {'we_art_id': 72, 'we_art_name': 'OH MY GIRL', 'url': 'url'},\n",
       " {'we_art_id': 73, 'we_art_name': 'CNBLUE', 'url': 'url'},\n",
       " {'we_art_id': 74, 'we_art_name': 'BILLLIE', 'url': 'url'},\n",
       " {'we_art_id': 75, 'we_art_name': 'B.I.G', 'url': 'url'},\n",
       " {'we_art_id': 76, 'we_art_name': 'WEVERSE ZONE', 'url': 'url'},\n",
       " {'we_art_id': 77, 'we_art_name': '3YE', 'url': 'url'},\n",
       " {'we_art_id': 78, 'we_art_name': 'AKMU', 'url': 'url'},\n",
       " {'we_art_id': 79, 'we_art_name': 'MINZY', 'url': 'url'},\n",
       " {'we_art_id': 80, 'we_art_name': 'KIM WOO SEOK', 'url': 'url'},\n",
       " {'we_art_id': 81, 'we_art_name': 'NEWJEANS', 'url': 'url'},\n",
       " {'we_art_id': 82, 'we_art_name': 'WHEE IN', 'url': 'url'},\n",
       " {'we_art_id': 83, 'we_art_name': 'B1A4', 'url': 'url'},\n",
       " {'we_art_id': 84, 'we_art_name': 'PARK BO YOUNG', 'url': 'url'},\n",
       " {'we_art_id': 85, 'we_art_name': 'LEEHI', 'url': 'url'},\n",
       " {'we_art_id': 86, 'we_art_name': 'BAMBAM', 'url': 'url'},\n",
       " {'we_art_id': 87, 'we_art_name': 'ONF', 'url': 'url'},\n",
       " {'we_art_id': 88, 'we_art_name': 'BIGBANG', 'url': 'url'},\n",
       " {'we_art_id': 89, 'we_art_name': 'KIM SEJEONG', 'url': 'url'},\n",
       " {'we_art_id': 90, 'we_art_name': 'VIXX', 'url': 'url'},\n",
       " {'we_art_id': 91, 'we_art_name': 'THE BOYZ', 'url': 'url'},\n",
       " {'we_art_id': 92, 'we_art_name': 'KIM SEON HO', 'url': 'url'},\n",
       " {'we_art_id': 93, 'we_art_name': \"E'LAST\", 'url': 'url'},\n",
       " {'we_art_id': 94, 'we_art_name': 'YURINA HIRATE', 'url': 'url'},\n",
       " {'we_art_id': 95, 'we_art_name': 'DRIPPIN', 'url': 'url'},\n",
       " {'we_art_id': 96, 'we_art_name': 'PENTAGON', 'url': 'url'},\n",
       " {'we_art_id': 97, 'we_art_name': 'BTOB', 'url': 'url'},\n",
       " {'we_art_id': 98, 'we_art_name': 'MOONBIN&SANHA', 'url': 'url'},\n",
       " {'we_art_id': 99, 'we_art_name': 'KWON EUN BI', 'url': 'url'},\n",
       " {'we_art_id': 100, 'we_art_name': '(G)I-DLE', 'url': 'url'},\n",
       " {'we_art_id': 101, 'we_art_name': 'AKB48', 'url': 'url'},\n",
       " {'we_art_id': 102, 'we_art_name': 'BOYNEXTDOOR', 'url': 'url'},\n",
       " {'we_art_id': 103, 'we_art_name': 'CLASS:Y', 'url': 'url'},\n",
       " {'we_art_id': 104, 'we_art_name': 'MOONCHILD', 'url': 'url'},\n",
       " {'we_art_id': 105, 'we_art_name': nan, 'url': 'url'},\n",
       " {'we_art_id': 106, 'we_art_name': 'LIGHTSUM', 'url': 'url'},\n",
       " {'we_art_id': 107, 'we_art_name': 'LEE SOO HYUK', 'url': 'url'},\n",
       " {'we_art_id': 108, 'we_art_name': 'LEE JIN HYUK', 'url': 'url'},\n",
       " {'we_art_id': 109, 'we_art_name': 'IMASE', 'url': 'url'},\n",
       " {'we_art_id': 110, 'we_art_name': 'SON NA EUN', 'url': 'url'},\n",
       " {'we_art_id': 111, 'we_art_name': 'ILLIT', 'url': 'url'},\n",
       " {'we_art_id': 112, 'we_art_name': 'LUN8', 'url': 'url'},\n",
       " {'we_art_id': 113, 'we_art_name': 'KIM JAE JOONG', 'url': 'url'},\n",
       " {'we_art_id': 114, 'we_art_name': 'CHOI SOO HO', 'url': 'url'},\n",
       " {'we_art_id': 115, 'we_art_name': 'CHUU', 'url': 'url'},\n",
       " {'we_art_id': 116, 'we_art_name': 'HI-FI UN!CORN', 'url': 'url'},\n",
       " {'we_art_id': 117, 'we_art_name': 'JANG KI YONG', 'url': 'url'},\n",
       " {'we_art_id': 118, 'we_art_name': 'WEKI MEKI', 'url': 'url'},\n",
       " {'we_art_id': 119, 'we_art_name': 'KATSEYE', 'url': 'url'},\n",
       " {'we_art_id': 120, 'we_art_name': 'CHANMINA', 'url': 'url'},\n",
       " {'we_art_id': 121, 'we_art_name': 'KIM MYUNGSOO(L)', 'url': 'url'},\n",
       " {'we_art_id': 122, 'we_art_name': 'KANGTA', 'url': 'url'},\n",
       " {'we_art_id': 123, 'we_art_name': 'BOA', 'url': 'url'},\n",
       " {'we_art_id': 124, 'we_art_name': 'TVXQ!', 'url': 'url'},\n",
       " {'we_art_id': 125, 'we_art_name': 'SUPER JUNIOR', 'url': 'url'},\n",
       " {'we_art_id': 126, 'we_art_name': \"GIRLS' GENERATION\", 'url': 'url'},\n",
       " {'we_art_id': 127, 'we_art_name': 'SHINEE', 'url': 'url'},\n",
       " {'we_art_id': 128, 'we_art_name': 'RED VELVET', 'url': 'url'},\n",
       " {'we_art_id': 129, 'we_art_name': 'EXO', 'url': 'url'},\n",
       " {'we_art_id': 130, 'we_art_name': 'NCT 127', 'url': 'url'},\n",
       " {'we_art_id': 131, 'we_art_name': 'NCT DREAM', 'url': 'url'},\n",
       " {'we_art_id': 132, 'we_art_name': 'WAYV', 'url': 'url'},\n",
       " {'we_art_id': 133, 'we_art_name': 'AESPA', 'url': 'url'},\n",
       " {'we_art_id': 134, 'we_art_name': 'RIIZE', 'url': 'url'},\n",
       " {'we_art_id': 135, 'we_art_name': 'SMTOWN', 'url': 'url'},\n",
       " {'we_art_id': 136, 'we_art_name': 'SM_CONCERT', 'url': 'url'},\n",
       " {'we_art_id': 137, 'we_art_name': 'LEE SUNG KYOUNG', 'url': 'url'},\n",
       " {'we_art_id': 138, 'we_art_name': 'AHN HYO SEOP', 'url': 'url'},\n",
       " {'we_art_id': 139, 'we_art_name': 'QWER', 'url': 'url'},\n",
       " {'we_art_id': 140, 'we_art_name': 'NCT WISH', 'url': 'url'},\n",
       " {'we_art_id': 141, 'we_art_name': 'WHIB', 'url': 'url'},\n",
       " {'we_art_id': 142, 'we_art_name': 'AMPERS&ONE', 'url': 'url'},\n",
       " {'we_art_id': 143, 'we_art_name': 'WEVERSE CON FESTIVAL', 'url': 'url'},\n",
       " {'we_art_id': 1593, 'we_art_name': 'KYUHYUN', 'url': 'url'},\n",
       " {'we_art_id': 1594, 'we_art_name': 'JD1', 'url': 'url'},\n",
       " {'we_art_id': 1595, 'we_art_name': 'BULLET TRAIN', 'url': 'url'},\n",
       " {'we_art_id': 1596, 'we_art_name': 'SUPER★DRAGON', 'url': 'url'},\n",
       " {'we_art_id': 1597, 'we_art_name': \"ONE N' ONLY\", 'url': 'url'},\n",
       " {'we_art_id': 1598, 'we_art_name': 'LIENEL', 'url': 'url'},\n",
       " {'we_art_id': 1629, 'we_art_name': 'PLAVE', 'url': 'url'},\n",
       " {'we_art_id': 1630, 'we_art_name': 'TWS', 'url': 'url'},\n",
       " {'we_art_id': 1631, 'we_art_name': 'BABYMONSTER', 'url': 'url'},\n",
       " {'we_art_id': 1632, 'we_art_name': nan, 'url': 'url'},\n",
       " {'we_art_id': 1633, 'we_art_name': nan, 'url': 'url'},\n",
       " {'we_art_id': 1634, 'we_art_name': 'DARK MOON', 'url': 'url'},\n",
       " {'we_art_id': 1635, 'we_art_name': 'NIGHTLY', 'url': 'url'},\n",
       " {'we_art_id': 1636, 'we_art_name': 'BANG YEDAM', 'url': 'url'},\n",
       " {'we_art_id': 1637, 'we_art_name': 'LUCAS', 'url': 'url'},\n",
       " {'we_art_id': 1638, 'we_art_name': 'CONAN GRAY', 'url': 'url'},\n",
       " {'we_art_id': 1639, 'we_art_name': 'UNIS', 'url': 'url'},\n",
       " {'we_art_id': 1640, 'we_art_name': '리얼 환경 테스트용', 'url': 'url'},\n",
       " {'we_art_id': 1641, 'we_art_name': 'THUY', 'url': 'url'},\n",
       " {'we_art_id': 1642, 'we_art_name': 'NOWADAYS', 'url': 'url'},\n",
       " {'we_art_id': 1643, 'we_art_name': 'UMI', 'url': 'url'},\n",
       " {'we_art_id': 1644, 'we_art_name': 'YUNGBLUD', 'url': 'url'},\n",
       " {'we_art_id': 1645, 'we_art_name': 'SYUDOU', 'url': 'url'},\n",
       " {'we_art_id': 1646, 'we_art_name': 'AYUMU IMAZU', 'url': 'url'},\n",
       " {'we_art_id': 1647, 'we_art_name': nan, 'url': 'url'},\n",
       " {'we_art_id': 1648, 'we_art_name': 'NOA', 'url': 'url'},\n",
       " {'we_art_id': 1649, 'we_art_name': 'GIRLS ON FIRE', 'url': 'url'},\n",
       " {'we_art_id': 1650, 'we_art_name': 'KIM WOOJIN', 'url': 'url'},\n",
       " {'we_art_id': 1651, 'we_art_name': 'EILL', 'url': 'url'},\n",
       " {'we_art_id': 1652, 'we_art_name': 'ONEW', 'url': 'url'},\n",
       " {'we_art_id': 1653, 'we_art_name': 'LAUV', 'url': 'url'},\n",
       " {'we_art_id': 1654, 'we_art_name': 'VVUP', 'url': 'url'},\n",
       " {'we_art_id': 1655, 'we_art_name': '10CM', 'url': 'url'},\n",
       " {'we_art_id': 1656, 'we_art_name': 'BIBI', 'url': 'url'},\n",
       " {'we_art_id': 1657, 'we_art_name': 'KINO', 'url': 'url'},\n",
       " {'we_art_id': 1658, 'we_art_name': '82MAJOR', 'url': 'url'},\n",
       " {'we_art_id': 1659, 'we_art_name': 'HONG SEOK', 'url': 'url'},\n",
       " {'we_art_id': 1660, 'we_art_name': 'YOASOBI', 'url': 'url'},\n",
       " {'we_art_id': 1661, 'we_art_name': 'BYEON WOO SEOK', 'url': 'url'},\n",
       " {'we_art_id': 1662, 'we_art_name': 'WOOSEOK', 'url': 'url'},\n",
       " {'we_art_id': 1663, 'we_art_name': 'CHEN', 'url': 'url'},\n",
       " {'we_art_id': 1664, 'we_art_name': 'BAEKHYUN', 'url': 'url'},\n",
       " {'we_art_id': 1665, 'we_art_name': 'XIUMIN', 'url': 'url'},\n",
       " {'we_art_id': 1666, 'we_art_name': \"WOLF'LO\", 'url': 'url'},\n",
       " {'we_art_id': 1667, 'we_art_name': 'TIOT', 'url': 'url'},\n",
       " {'we_art_id': 1669, 'we_art_name': 'YOUNGTAK', 'url': 'url'},\n",
       " {'we_art_id': 1670, 'we_art_name': 'ARIANA GRANDE', 'url': 'url'},\n",
       " {'we_art_id': 1671, 'we_art_name': 'JVKE', 'url': 'url'},\n",
       " {'we_art_id': 1672, 'we_art_name': nan, 'url': 'url'},\n",
       " {'we_art_id': 1673, 'we_art_name': 'JEONG SUN AH', 'url': 'url'},\n",
       " {'we_art_id': 1674, 'we_art_name': nan, 'url': 'url'},\n",
       " {'we_art_id': 65, 'we_art_name': nan, 'url': 'url'},\n",
       " {'we_art_id': 66, 'we_art_name': 'DEMI LOVATO', 'url': 'url'},\n",
       " {'we_art_id': 67, 'we_art_name': 'JUSTIN BIEBER', 'url': 'url'},\n",
       " {'we_art_id': 68, 'we_art_name': 'THE KID LAROI', 'url': 'url'},\n",
       " {'we_art_id': 70, 'we_art_name': '물지않아요', 'url': 'url'},\n",
       " {'we_art_id': 11, 'we_art_name': 'HENRY_사용금지', 'url': 'url'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "we_artist2.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "\t  # # NamuLoader 를 사용해서 url 정보를 크롤링한다.\n",
    "    # url = 'https://namu.wiki/w/(%EC%97%AC%EC%9E%90)%EC%95%84%EC%9D%B4%EB%93%A4?from=%EC%97%AC%EC%9E%90%EC%95%84%EC%9D%B4%EB%93%A4'\n",
    "    # max_hop = 1\n",
    "    # verbose = True\n",
    "    # loader = NamuLoader(url=url, max_hop=max_hop, verbose=verbose)\n",
    "\n",
    "\t\t# # 크롤링한 데이터를 documents 에 append \n",
    "    # documents = []\n",
    "    # for doc in loader.lazy_load():\n",
    "    #     documents.append({\n",
    "    #         \"page_content\": doc.page_content,\n",
    "    #         \"metadata\": doc.metadata\n",
    "    #     })\n",
    "    \n",
    "    # 내가 작업하고자 하는 GCP 프로젝트, region, dataset, table id 설정\n",
    "    PROJECT_ID = \"wev-dev-analytics\"\n",
    "    REGION = \"asia-northeast3\"\n",
    "    DATASET_ID = \"namu_wiki\"\n",
    "    TABLE_ID = \"art_info_url\"\n",
    "\n",
    "    # 빅쿼리에 저장할 테이블의 schema 정의\n",
    "    client = bigquery.Client(project=PROJECT_ID)\n",
    "    schema = [\n",
    "      bigquery.SchemaField(\"we_art_id\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"we_art_name\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"url\", \"STRING\"),\n",
    "      ]\n",
    "    \n",
    "    dataset_ref = client.dataset(DATASET_ID)\n",
    "    dataset = bigquery.Dataset(dataset_ref)\n",
    "    dataset.location = REGION\n",
    "\n",
    "    # 데이터셋 생성 (이미 존재하는 경우 생략)\n",
    "    try:\n",
    "        client.create_dataset(dataset)\n",
    "        print(f\"Created dataset {DATASET_ID} in {REGION}\")\n",
    "    except:\n",
    "        print(f\"Dataset {DATASET_ID} already exists in {REGION}\")\n",
    "    \n",
    "    # 테이블 생성 (이미 존재하는 경우 생략)\n",
    "    table_ref = dataset_ref.table(TABLE_ID)\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "\n",
    "    try:\n",
    "        client.create_table(table)\n",
    "        print(f\"Created table {TABLE_ID} in dataset {DATASET_ID}\")\n",
    "    except:\n",
    "        print(f\"Table {TABLE_ID} already exists in dataset {DATASET_ID}\")\n",
    "\n",
    "\t\t# # 넣고 싶은 ARTIST_INFO, PAGE_URL 값을 기입해준다.\n",
    "    # ARTIST_INFO = \"(여자)아이들\"\n",
    "    # PAGE_URL = url\n",
    "\n",
    "\t\t# 각 파라미터를 기입해준다. WRITE_APPEND 은 테이블에 데이터가 append 되고, WRITE_TRUNCATE 을 기입하면 overwrite 된다.\n",
    "    load_data_to_bigquery(client, documents, PROJECT_ID, DATASET_ID, TABLE_ID, REGION, bigquery.WriteDisposition.WRITE_APPEND) # WRITE_APPEND, WRITE_TRUNCATE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
