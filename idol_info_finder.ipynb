{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # .env 파일에서 환경 변수를 불러옵니다.\n",
    "\n",
    "# api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# print(f\"API Key: {api_key}\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "search_engine_key = os.getenv(\"GOOGLE_SEARCH_ENGINE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "urls = {\n",
    "    \"namu_crawler.py\":\"https://raw.githubusercontent.com/lymanstudio/proj_artist_info_gen/main/namu_crawler.py\",\n",
    "    \"namu_loader.py\":\"https://raw.githubusercontent.com/lymanstudio/proj_artist_info_gen/main/namu_loader.py\"\n",
    "}\n",
    "\n",
    "for key, val in urls.items():    \n",
    "    r = requests.get(val)\n",
    "\n",
    "    # 다운로드한 파일을 현재 디렉토리에 저장\n",
    "    with open(key, \"w\", encoding='utf-8') as file:\n",
    "        file.write(r.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bigquery에서 사전에 저장한 URL 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login&state=Wrh1jjvbHkinoL026oUmdeFe0EOQe8&access_type=offline&code_challenge=d8G031ir-pnumXo7pDZ5IL5iOQ3DKeZpQOkWFSxQWxA&code_challenge_method=S256\n",
      "\n",
      "\n",
      "Credentials saved to file: [C:\\Users\\sanghyoon\\AppData\\Roaming\\gcloud\\application_default_credentials.json]\n",
      "\n",
      "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
      "\n",
      "Quota project \"wev-dev-analytics\" was added to ADC which can be used by Google client libraries for billing and quota. Note that some services may still bill the project owning the resource.\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth application-default login\n",
    "# %pip install --upgrade google-cloud-bigquery pandas db-dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 177 rows from /projects/wev-dev-analytics/datasets/namu_wiki/tables/art_info_url\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sanghyoon\\Documents\\Github\\idol_info_finder\\idol_info_finder-2\\venv\\lib\\site-packages\\google\\cloud\\bigquery\\table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  we_art_id we_art_name                      url\n",
      "0         7          CL   https://namu.wiki/w/CL\n",
      "1        45          XG   https://namu.wiki/w/XG\n",
      "2        77         3YE  https://namu.wiki/w/3YE\n",
      "3       129         EXO  https://namu.wiki/w/EXO\n",
      "4      1648         NOA  https://namu.wiki/w/NOA\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "\n",
    "# 내가 작업하고자 하는 GCP 프로젝트, region, dataset, table id 설정\n",
    "PROJECT_ID = \"wev-dev-analytics\"\n",
    "REGION = \"asia-northeast3\"\n",
    "DATASET_ID = \"namu_wiki\"\n",
    "TABLE_ID = \"art_info_url\"\n",
    "\n",
    "# Initialize a BigQuery client\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# Define the dataset and table you want to import\n",
    "table_ref = client.dataset(DATASET_ID).table(TABLE_ID)\n",
    "\n",
    "# Query the table and convert it to a DataFrame\n",
    "def import_table_to_dataframe(client, table_ref):\n",
    "    # Construct a BigQuery client object.\n",
    "    table = client.get_table(table_ref)  # API call\n",
    "\n",
    "    print(f\"Downloading {table.num_rows} rows from {table_ref.path}\")\n",
    "\n",
    "    # Load the table into a Pandas DataFrame\n",
    "    query = f'SELECT * FROM `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`'\n",
    "    dataframe = client.query(query).to_dataframe()\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "# Import the table to a DataFrame\n",
    "urls = import_table_to_dataframe(client, table_ref)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(urls.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL에 저장된 데이터 크롤링 후 빅쿼리 테이블에 적재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#코드 흐름\n",
    "#나무로더 인스턴스 생성\n",
    "#위키페이지 크롤링\n",
    "#테이블에 저장\n",
    "#루프로 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from namu_loader import NamuLoader\n",
    "import textwrap\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "import openai\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "# from langchain_google_vertexai import VertexAIEmbeddings\n",
    "# from langchain_google_community import BigQueryVectorStore\n",
    "\n",
    "# BigQuery\n",
    "def load_data_to_bigquery(client, json_data, project_id, dataset_id, table_id, region, write_disposition, artist_info, page_url):\n",
    "    \n",
    "    # print(json_data)\n",
    "    \n",
    "    # metadata 는 한글이 섞여있으므로 ensure_ascii 옵션을 False 로 설정한다.\n",
    "    # artist_info, page_url 은 크롤링된 정보에서 가져오는 것이 아니므로 수동으로 넣어준다.\n",
    "    for item in json_data:\n",
    "        item['metadata'] = json.dumps(item['metadata'], ensure_ascii=False)\n",
    "        item['artist_info'] = artist_info\n",
    "        item['page_url'] = page_url\n",
    "    \n",
    "    table_ref = client.dataset(dataset_id, project=project_id).table(table_id)\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.write_disposition = write_disposition\n",
    "    \n",
    "    load_job = client.load_table_from_json(\n",
    "        json_data, table_ref, location=region, job_config=job_config\n",
    "    )\n",
    "    \n",
    "    load_job.result()  \n",
    "    print(f'Loaded {len(json_data)} rows into {project_id}:{dataset_id}.{table_id}')\n",
    "\n",
    "def push_art_info_to_bigquery(art_info, max_hop = 1):\n",
    "\n",
    "\t  # NamuLoader 를 사용해서 url 정보를 크롤링한다.\n",
    "    # url = 'https://namu.wiki/w/(%EC%97%AC%EC%9E%90)%EC%95%84%EC%9D%B4%EB%93%A4?from=%EC%97%AC%EC%9E%90%EC%95%84%EC%9D%B4%EB%93%A4'\n",
    "    # max_hop = 1\n",
    "    print(art_info['we_art_name'].iat[0],' : ',art_info['url'].iat[0])\n",
    "    url = art_info['url'].iat[0]\n",
    "    verbose = True\n",
    "    loader = NamuLoader(url=url, max_hop=max_hop, verbose=verbose)\n",
    "    # print(url)\n",
    "\n",
    "\t\t# 크롤링한 데이터를 documents 에 append \n",
    "    documents = []\n",
    "    for doc in loader.lazy_load():\n",
    "        documents.append({\n",
    "            \"page_content\": doc.page_content,\n",
    "            \"metadata\": doc.metadata\n",
    "        })\n",
    "    \n",
    "    # 내가 작업하고자 하는 GCP 프로젝트, region, dataset, table id 설정\n",
    "    PROJECT_ID = \"wev-dev-analytics\"\n",
    "    REGION = \"asia-northeast3\"\n",
    "    DATASET_ID = \"namu_wiki\"\n",
    "    TABLE_ID = \"namu_string_result\"\n",
    "\n",
    "    # 빅쿼리에 저장할 테이블의 schema 정의\n",
    "    client = bigquery.Client(project=PROJECT_ID)\n",
    "    schema = [\n",
    "      bigquery.SchemaField(\"page_url\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"artist_info\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"metadata\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"page_content\", \"STRING\"),\n",
    "      ]\n",
    "    \n",
    "    dataset_ref = client.dataset(DATASET_ID)\n",
    "    dataset = bigquery.Dataset(dataset_ref)\n",
    "    dataset.location = REGION\n",
    "\n",
    "    # 데이터셋 생성 (이미 존재하는 경우 생략)\n",
    "    try:\n",
    "        client.create_dataset(dataset)\n",
    "        print(f\"Created dataset {DATASET_ID} in {REGION}\")\n",
    "    except:\n",
    "        print(f\"Dataset {DATASET_ID} already exists in {REGION}\")\n",
    "    \n",
    "    # 테이블 생성 (이미 존재하는 경우 생략)\n",
    "    table_ref = dataset_ref.table(TABLE_ID)\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "\n",
    "    try:\n",
    "        client.create_table(table)\n",
    "        print(f\"Created table {TABLE_ID} in dataset {DATASET_ID}\")\n",
    "    except:\n",
    "        print(f\"Table {TABLE_ID} already exists in dataset {DATASET_ID}\")\n",
    "\n",
    "\t\t# 넣고 싶은 ARTIST_INFO, PAGE_URL 값을 기입해준다.\n",
    "    ARTIST_INFO = art_info['we_art_name'].iat[0]\n",
    "    PAGE_URL = url\n",
    "\n",
    "\t\t# 각 파라미터를 기입해준다. WRITE_APPEND 은 테이블에 데이터가 append 되고, WRITE_TRUNCATE 을 기입하면 overwrite 된다.\n",
    "    load_data_to_bigquery(client, documents, PROJECT_ID, DATASET_ID, TABLE_ID, REGION, bigquery.WriteDisposition.WRITE_APPEND, ARTIST_INFO, PAGE_URL) # WRITE_APPEND, WRITE_TRUNCATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'iterrow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m \u001b[43murls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterrow\u001b[49m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(url)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# push_art_info_to_bigquery(url[url['we_art_name'] == 'WINNER'])\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sanghyoon\\Documents\\Github\\idol_info_finder\\idol_info_finder-2\\venv\\lib\\site-packages\\pandas\\core\\generic.py:6299\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   6293\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   6294\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   6295\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   6296\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   6297\u001b[0m ):\n\u001b[0;32m   6298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 6299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'iterrow'"
     ]
    }
   ],
   "source": [
    "for url in urls.iterrow:\n",
    "    print(url)\n",
    "    # push_art_info_to_bigquery(url[url['we_art_name'] == 'WINNER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
